<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Aniruddha Deb - Mathematics</title><link href="https://aniruddha-deb.github.io/" rel="alternate"></link><link href="https://aniruddha-deb.github.io/feeds/mathematics.atom.xml" rel="self"></link><id>https://aniruddha-deb.github.io/</id><updated>2021-12-25T15:20:00+05:30</updated><entry><title>A Note on Conditional Probability</title><link href="https://aniruddha-deb.github.io/articles/2021/note-on-conditional-probability.html" rel="alternate"></link><published>2021-12-25T15:20:00+05:30</published><updated>2021-12-25T15:20:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2021-12-25:/articles/2021/note-on-conditional-probability.html</id><summary type="html">&lt;p&gt;$\newcommand{\cE}[2]{\mathbf{E}(#1\ |\ #2)}$$\newcommand{\cP}[2]{\mathbf{P}(#1\ |\ #2)}$$\renewcommand{\P}[1]{\mathbf{P}(#1)}$$\newcommand{\E}[1]{\mathbf{E}(#1)}$$\newcommand{\F}{\mathcal{F}}$$\newcommand{\G}{\mathcal{G}}$$\newcommand{\ind}[1]{\mathbf{1}_{#1}}$
To motivate this note, I’ll pose the following …&lt;/p&gt;</summary><content type="html">&lt;p&gt;$\newcommand{\cE}[2]{\mathbf{E}(#1\ |\ #2)}$$\newcommand{\cP}[2]{\mathbf{P}(#1\ |\ #2)}$$\renewcommand{\P}[1]{\mathbf{P}(#1)}$$\newcommand{\E}[1]{\mathbf{E}(#1)}$$\newcommand{\F}{\mathcal{F}}$$\newcommand{\G}{\mathcal{G}}$$\newcommand{\ind}[1]{\mathbf{1}_{#1}}$
To motivate this note, I’ll pose the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider $X \sim \text{Uniform}([0,1])$. What is $\cP{X &amp;gt; \frac{1}{2}}{X \in \mathbb{Q}}$ ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At first glance, the answer seems simple: it’s 1/2! Closer inspection reveals that the event $X \in \mathbb{Q}$ is not measurable: the rationals have measure zero, hence from our high-school (perhaps even undergraduate) definition of conditional probability, the given probability should be undefined. However, the conditional probability does exist, and it equals 1/2.&lt;/p&gt;
&lt;h2&gt;Redefining Conditional Probability&lt;/h2&gt;
&lt;p&gt;To account for events of measure zero, we must first redefine $\cP{A}{B}$. So far, we've defined this as $\cP{A}{B} = \P{A \cap B}\ /\ \P{B}$, $\P{B} \ne 0$. We overcome the case where $\P{B} = 0$ by defining &lt;strong&gt;conditional probability as a random variable that satisfies some properties&lt;/strong&gt;. The intuition behind this is that if the conditional probability doesn't exist by the previous definition, we can use expectation to say that it 'might' exist and be equal to some expected value. This also nicely sets us up for conditional expectation and variance, and the law of total expectation and law of total variance. Before formally defining conditional probability, Recall the following definitions first:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: A Sub-$\sigma$ algebra of $\F$ is a $\sigma$ algebra $\G$ such that $\G \subset \F$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: A Measurable Function $f: (X,\Sigma) \to (Y,\Gamma)$ between two measurable spaces is a function such that for every $E \in \Gamma$, $\{x \in X\ |\ f(x) \in E\} \in \Sigma$. If $(Y, \Gamma) = (\mathbb{R}, \mathcal{B}(\mathbb{R}))$, then $f$ is said to be $\Sigma$-measurable&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: The $\sigma$-algebra generated by a function $f: (X,\Sigma) \to (Y, \Gamma)$ is the collection of all inverse images $f^{-1}(S),\ S \in \Gamma$. 
$$\sigma(f) := \{ f^{-1}(S) : S \in \Gamma \}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With these three definitions, we define the conditional probability function on a sub-$\sigma$ algebra $\G$ as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: The conditional probability $\cP{\cdot}{\G}$ is a $\G$-measurable random variable such that $$\E{\cP{A}{\G}\ind{G}} = \P{A \cap G} \quad \forall\ G \in \G$$ In terms of integrals, we can rewrite this as $$\int_G \cP{A}{\G} d\mathbf{P} = \P{A \cap G}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Defining the conditioning on a sub-$\sigma$ algebra rather than an event or another random variable has the following benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we want to condition on a random variable $X$, then $\G = \sigma(X)$&lt;/li&gt;
&lt;li&gt;If we want to condition on multiple random variables $X_1, X_2, \ldots$, then $\G = \sigma(X_1, X_2, \ldots)$ where $\sigma(X_1, X_2) = \{ \{X_1 \le a\} \cap \{X_2 \le b\}\ |\ a,b \in \mathbb{R}\}$ (can be extended to a countable sequence of random variables)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, this definition of conditional probability is the most general definition.&lt;/p&gt;
&lt;h2&gt;Existence and Uniqueness of Conditional Probability&lt;/h2&gt;
&lt;p&gt;From the integral definition, it may be obvious that &lt;strong&gt;conditional probability is unique up to a set of measure zero&lt;/strong&gt;, that is, if $\P{A} = 0$, then $\cP{A}{\G}$ can take any value. This leads to the problem of &lt;em&gt;regularized conditional probability&lt;/em&gt;. Essentially, we define values for sets whose measure is zero such that the resulting conditional probability function is 'nice', ie measurable. This would need us to set up constructs such as a &lt;a href="https://en.wikipedia.org/wiki/Radon_space"&gt;Polish Space&lt;/a&gt; and then use some functional analysis results, but we'll not get into that in this article.&lt;/p&gt;
&lt;p&gt;Existence can be proved in many ways: here, we use the Radon-Nikodym theorem. I won't be proving the theorem, but I'll quote it, along with some exposition, which should be sufficient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: A measure $\mu$ is said to be &lt;em&gt;absolutely continuous&lt;/em&gt; with respect to another measure $\lambda$ (both defined on the same $\sigma$-algebra) if for every measurable set $A$, $\lambda(A) = 0 \implies \mu(A) = 0$. We denote this as $\mu \ll \lambda$, and we say that $\lambda$ &lt;em&gt;dominates&lt;/em&gt; $\mu$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem (Radon-Nikodym):&lt;/strong&gt; if $\mu$ and $\lambda$ are defined on $(X,\Sigma)$ such that $mu \ll \lambda$, then there exists a $\Sigma$-measurable function $f$ such that for any $A \in \Sigma$, $$\mu(A) = \int_A f\ d\lambda$$
The function $f$ is called the &lt;em&gt;radon-nikodym&lt;/em&gt; derivative of $\mu$ and is denoted as $d\mu/d\lambda$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We now prove our claim&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; $\cP{A}{\G}$ exists and is unique upto a set of probability 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Define $\cP{A}{\G}$ to simply be $d\nu/d\mathbf{P}_\G$, where $\mathbf{P}_\G$ is simply $\mathbf{P}$ restricted to $\G$, and $$\nu(E) = \P{A \cap E}$$ Note $\nu \ll \mathbf{P}_\G$ and by the radon-nikodym theorem, $\cP{A}{\G}$ exists and is unique upto a set of probability 0. Note that defining $\cP{A}{\G}$ like so also takes care of the definition, as $$\E{\cP{A}{\G}\ind{G}} = \int_G \frac{d\nu}{d\mathbf{P}_\G}\ d\mathbf{P}_\G = \nu(G) = \P{A \cap G}$$&lt;/p&gt;
&lt;h2&gt;Conditional Expectation, and deriving Conditional Probability from Conditional Expectation&lt;/h2&gt;
&lt;p&gt;Conditional expectation is defined in much the same way: as a random variable satisfying the property $\E{\cE{Y}{\G}\ind{G}} = \E{Y \ind{G}}$. In a handwavy sense, conditional expectation is 'the best approximation' of the specified quantity, given the information we have from $\G$. There's a very nice functional analysis proof of this which treats random variables as vectors in a functional space and then approximates them using expectation, but I'll leave that out for now.&lt;/p&gt;
&lt;p&gt;Most of the same properties as conditional probability apply to conditional expectation as well, just that in the proof of existence, since expectation can be negative and we need a positive value for it to be a measure, we define $\cE{A}{\G}$ as $d\E{A^+ \ind{G}}/d\mathbf{P}_\G - d\E{A^- \ind{G}}/d\mathbf{P}_\G$ and proceed similarly. &lt;/p&gt;
&lt;p&gt;In several textbooks, conditional expectation is derived first, and conditional probability follows. This is because $\cP{A}{\G} = \cE{\ind{A}}{\G}$. Also, a neat proof involving functional analysis is used to prove the existence of conditional expectation: $\cE{Y}{\G}$ is the orthogonal projection of $Y$ onto the closed subspace $\mathbb{L}^2(\G)$, and from results about hilbert space, we can prove this.&lt;/p&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Now that we have this definition, how do we apply it? Sadly, we can't directly obtain the conditional probability from it (unless we evaluate the radon-nikodym derivative), and the best we can do is assume that the conditional probability is a reasonable random variable, and then verify that it satisfies the definition.&lt;/p&gt;
&lt;p&gt;For the given problem, $\G = \sigma(\mathbb{Q})$. This is because for $\cP{\{X &amp;gt; x\}}{\G}$ to be $\G$-measurable, $G$ must have all the information we're given, namely whether $X$ is rational or not, hence the specification of the sub-$\sigma$ algebra as mentioned. You can also think of it from the best approximation perspective: we're only given information about whether X is rational or not and about X's magnitude, and both those quantities are expressed in this sub-$\sigma$ algebra. Now, let $\cP{\{X &amp;gt; x\}}{\G} = 1-x$. This is $\G$-measurable (the preimage of any borel set lies in $\G$). Verifying the two integrals gives us
$$\begin{align}
\P{\{X &amp;gt; x\} \cup G} &amp;amp;= \int_G \int_x^1 1\ d\mathbf{P}\ d\mathbf{P_0}\\
&amp;amp;= \int_G 1-x\ d\mathbf{P_0}
\end{align}$$
$$\E{\cP{\{X &amp;gt; x\}}{\G}\ind{G}} = \int_G 1-x\ d\mathbf{P_0}$$
Since both these integrals are equal for all $G \in \G$, we have $\cP{X &amp;gt; \frac{1}{2}}{X \in \mathbb{Q}} = 1/2$.&lt;/p&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Rosenthal, Jeffrey S. &lt;em&gt;A First Look at Rigorous Probability Theory&lt;/em&gt;. 2nd ed, World Scientific, 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://math.stackexchange.com/questions/2306986/uniqueness-of-conditional-expectation"&gt;https://math.stackexchange.com/questions/2306986/uniqueness-of-conditional-expectation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/395310/what-does-conditioning-on-a-random-variable-mean"&gt;https://stats.stackexchange.com/questions/395310/what-does-conditioning-on-a-random-variable-mean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.stat.berkeley.edu/users/pitman/s205f02/lecture15.pdf"&gt;https://www.stat.berkeley.edu/users/pitman/s205f02/lecture15.pdf&lt;/a&gt; (this contains the projection proof)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.cmu.edu/~arinaldo/Teaching/36752/S18/Notes/lec_notes_6.pdf"&gt;http://www.stat.cmu.edu/~arinaldo/Teaching/36752/S18/Notes/lec_notes_6.pdf&lt;/a&gt; (this too has the projection proof)&lt;/li&gt;
&lt;li&gt;Billingsley, Patrick. Probability and Measure. 3rd ed, Wiley, 1995.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;PS&lt;/h3&gt;
&lt;p&gt;Merry christmas to you if you're reading this! A junior emailed me a while back, stating that he read my blog and couldn't grasp at times why there are &lt;em&gt;random mathematical stuffs&lt;/em&gt;. In my reply, the blog was supposed to be a math/programming blog with the occasional &lt;em&gt;random personal stuff&lt;/em&gt;, but the tables seem to have turned :P&lt;/p&gt;
&lt;p&gt;I think a good new year's resolution would be to learn one new math topic every week and then blog about it. Technical blogging as of now takes a lot of time for me (this article took the good part of a day), and I think as I write and learn more, that time should go down. &lt;/p&gt;
&lt;p&gt;Here's to a more mathematical 2022!&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>A Note on Random Variables</title><link href="https://aniruddha-deb.github.io/articles/2021/note-on-random-variables.html" rel="alternate"></link><published>2021-12-03T21:00:00+05:30</published><updated>2021-12-03T21:00:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2021-12-03:/articles/2021/note-on-random-variables.html</id><summary type="html">&lt;p&gt;$\newcommand{\triple}{(\Omega, \mathcal{F}, \mathbf{P})}$$\newcommand{\P}{\mathbf{P}}$
This note on random variables follows as a result of confusing notation in several math textbooks. I'll explain random variables (in measure theoretic terms) as verbosely as I can, and then prove some results. This article assumes that the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;$\newcommand{\triple}{(\Omega, \mathcal{F}, \mathbf{P})}$$\newcommand{\P}{\mathbf{P}}$
This note on random variables follows as a result of confusing notation in several math textbooks. I'll explain random variables (in measure theoretic terms) as verbosely as I can, and then prove some results. This article assumes that the reader is familiar with probability triples $\triple$, as well as a basic idea of what random variables are, in non-measure theory terms.&lt;/p&gt;
&lt;h2&gt;1. Random Variable Prerequisites&lt;/h2&gt;
&lt;p&gt;We start with defining measurable spaces and measurable functions &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.1&lt;/strong&gt;: A Measurable space $(X,\Sigma)$ consists of a set $X$ and a $\sigma$-algebra $\Sigma$ defined on $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.2&lt;/strong&gt;: A Generated $\sigma$-algebra is the smallest $\sigma$-algebra containing a specified collection of sets. That is, if $A$ is a set of subsets of $X$, $\sigma(A)$ is the smallest sigma-algebra such that $A \subseteq \sigma(A)$. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.3&lt;/strong&gt;: A Measurable Function $f: (X,\Sigma) \to (Y,\Gamma)$ between two measurable spaces is a function such that for every $E \in \Gamma$, $\{x \in X\ |\ f(x) \in E\} \in \Sigma$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since the definition $\{x \in X\ |\ f(x) \in E\}$ is used so commonly in the context of measurable functions, this has a special notation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.4&lt;/strong&gt;: $$f^{-1}(E) := \{x \in X\ |\ f(x) \in E\}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;NOTE: The above definition is confusing, but is unfortunately the norm when dealing with measurable functions. &lt;span style="color: red;"&gt;In the context of measurable functions, $f^{-1}$ does not refer to the inverse of $f$ (which is a function from $Y \to X$), but rather the set of preimages of all the elements contained in a set in the sigma algebra.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Measurable functions can also be defined in terms of the $\sigma$-algebra generated by a &lt;em&gt;function&lt;/em&gt;, rather than that of a set&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.5&lt;/strong&gt;: The $\sigma$-algebra generated by a function $f: (X,\Sigma) \to (Y, \Gamma)$ is the collection of all inverse images $f^{-1}(S),\ S \in \Gamma$. 
$$\sigma(f) := \{ f^{-1}(S) : S \in \Gamma \}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;According to this definition, if $\sigma(f) \subseteq \Sigma$, then $f$ is a measurable function.&lt;/p&gt;
&lt;h2&gt;2. Random Variables&lt;/h2&gt;
&lt;p&gt;Random variables are unfortunately, neither random nor variables. This is the first of many misnomers that we encounter in their study. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 2.1&lt;/strong&gt;: A Random Variable $X$ defined on a probability triple $\triple$ is a measurable function $X : (\Omega, \mathcal{F}) \to (\mathbb{R}, \mathcal{B}(\mathbb{R}))$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In it's simplest terms, A random variable is simply a function from $\Omega \to \mathbb{R}$, obeying some 'nice' rules which allow us to use probability measures with it. These nice rules would come into play a bit later, after we first see how random variables and probability measures go hand in hand.&lt;/p&gt;
&lt;p&gt;Consider $$\begin{align}\Omega &amp;amp;= \{1,2,3\}\\ \mathcal{F} &amp;amp;= 2^{\Omega} \\ \mathbf{P}&amp;amp;:\mathcal{F} \to [0,1]\end{align}$$ such that $\mathbf{P}\{1\} = \mathbf{P}\{2\} = \mathbf{P}\{3\}$ (This is the discrete uniform probability space on $\{1,2,3\}$). Let our random variable $X: \Omega \to \mathbb{R}$ map $\{i\}$ to $i$, $i \in \{1,2,3\}$. A graphical depiction of this would look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="X" src="res/rv_x.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Now, suppose we had to calculate the probability that the random variable $X$ would be less than or equal to $2.5$. The probability of this event occuring is given by $\P\{\omega \in \Omega\ :\ X(\omega) \le 2.5\}$. From the inverse notation we developed in $\S$1.4, We can also write this as $\P(X^{-1}((-\infty, 2.5]))$. From the graph, we clearly see that $1$ and $2$ are the only elements in $\Omega$ that would be in this set, hence $\P\{1,2\} = 2/3$. This is how random variables and probability measures go hand in hand.&lt;/p&gt;
&lt;p&gt;Why then, do random variables need to be measurable functions? &lt;strong&gt;Note that the probability measure is only defined for sets in $\mathcal{F}$, and if $X$ is not measurable, we cannot find the probability of certain events associated with $X$&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;An example for this is to consider $$\begin{align}\Omega &amp;amp;= \{1,2,3\}\\ \mathcal{F'} &amp;amp;= \{\emptyset, \{1\}, \{2,3\}, \Omega\} \\ \mathbf{P}&amp;amp;:\mathcal{F'} \to [0,1]\end{align}$$ such that $\P\{1\} = 1/3$. Now consider $X': \Omega \to \mathbb{R}$ such that $X'(i) = i,\ i \in \Omega$. This is the same map as before. However, if we try to calculate the probability that $X$ is less than or equal to 2.5 now, &lt;strong&gt;we find that $\P\{1,2\}$ is undefined, as $\{1,2\} \not\in \mathcal{F'}$&lt;/strong&gt;. Hence, $X'$ is not a random variable, as it is not measurable on $(\Omega, \mathcal{F'})$. More specifically, $\sigma(X) = 2^\Omega \not\subseteq \mathcal{F'}$, hence, $X'$ is not measurable&lt;/p&gt;
&lt;h2&gt;3. Results on Random Variables&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim 3.1&lt;/strong&gt;: If $X: \Omega \to \mathbb(R)$ is a random variable on $\triple$, then $X^{-1}(B) = A \implies X^{-1}(B^C) = A^C$. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A simple (maybe even obvious) claim, the proof is by definition: $$\begin{align}
X^{-1}(B) &amp;amp;= A = \{\omega \in \Omega\ :\ X(\omega) \in B\} \\
\implies X^{-1}(B^C) &amp;amp;= \{\omega \in \Omega\ :\ X(\omega) \not\in B\} = A^C 
\end{align}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim 3.2&lt;/strong&gt;: If $X = \mathbf{1}_A$ is the indicator of some event $A \in \mathcal{F}$, then $X$ is a random variable&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof: for all $B \in \mathcal{B}(\mathbb{R})$, we have $X(B)$ equal to any one of $A$ (if $B$ contains 1 and not 0), $A^C$ (if $B$ contains 0 and not 1), $\emptyset$ (if $B$ contains neither 0 nor 1) or $\Omega$ (if $B$ contains both 0 and 1). Hence, $X$ is a random variable.&lt;/p&gt;
&lt;p&gt;The next two claims would be key to proving results about functions of random variables&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim 3.3&lt;/strong&gt;: if $f: (\Omega_1, \mathcal{F}_1) \to (\Omega_2, \mathcal{F}_2)$ and $g: (\Omega_2, \mathcal{F}_2) \to (\Omega_3, \mathcal{F}_3)$ are two measurable functions, then $f \circ g : (\Omega_1, \mathcal{F}_1) \to (\Omega_3, \mathcal{F}_3)$ is also a measurable function&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof: For all $B \in \mathcal{F}_3$, since $g$ is measurable, $g^{-1}(B) \in \mathcal{F}_2$. Since $f$ is measurable, $f^{-1}(g^{-1}(B)) \in \mathcal{F}_1$. Hence, $f\circ g$ is measurable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim 3.4&lt;/strong&gt;: $f: (\Omega_1, \mathcal{F}_1) \to (\Omega_2, \sigma(C))$ is measurable if $A \in C \implies f^{-1}(A) \in \mathcal{F}_1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof: Note that $f^{-1}(\Omega_2 \setminus A) = \Omega_2 \setminus f^{-1}(A)$, and $f^{-1}(\cup_n A_n) = \cup_n f^{-1}(A_n)$. This, along with the fact that $\mathcal{F}_1$ is a $\sigma$-algebra proves that $\{A : f^{-1}(A) \in \mathcal{F}_1\}$ is a $\sigma$-algebra containing $C$. Since $\sigma(C)$ is the smallest $\sigma$-algbra containing C, $\sigma(C)$ would be a subset of the above $\sigma$-algebra, hence the claim is true.&lt;/p&gt;
&lt;p&gt;This above claim ensures that we don't need to prove that every set of a $\sigma$-algebra has a preimage in the previous $\sigma$-algebra. Proving it for only the generating set is enough eg. for $\mathcal{B}(\mathbb{R})$, it's sufficient to show that only the open sets have a preimage, something that we'll use in the next proof.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim 3.5&lt;/strong&gt;: Every continuous function $f: \mathbb{R} \to \mathbb{R}$ is measurable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof: from (3.4), it's sufficient to prove that for every open set $A$, $f^{-1}(A) \in \mathcal{B}(\mathbb{R})$. This follows from the continuity of $f$: $f$ is continuous iff $G$ is open implies that $f^{-1}(G)$ is also open. Hence, $f$ is measurable.&lt;/p&gt;
&lt;p&gt;The above three claims give us the following very powerful result: &lt;strong&gt;every continuous function of a random variable is also a random variable&lt;/strong&gt;. We can make a stronger claim, after proving the following claims as well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim 3.6&lt;/strong&gt;: If $X$ and $Y$ are random variables on $\triple$, then $X+Y$ and $XY$ are random variables as well&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof: This cute proof comes from Rosenthal. It's sufficient to prove that $X+Y$ is a random variable on the collection of sets $(-\infty, x)$, as the generated $\sigma$-algebra of this collection is $\mathcal{B}(\mathbb{R})$. Hence, consider the set $\{\omega \in \Omega : X(\omega) + Y(\omega) &amp;lt; x\}$. From the density theorem, we can find a rational number in $(X, x-Y)$ (I've dropped the $(\omega)$, as it's implicit here). hence, $$\{X + Y &amp;lt; x\} = \bigcup_{\text{r rational}} (\{X \lt r\} \cap \{Y \lt x - r\})$$ Since all the elements in the union belong to $\mathcal{F}$ and since $\mathcal{F}$ is a $\sigma$-algebra, $X+Y$ is a random variable.&lt;/p&gt;
&lt;p&gt;XY is also a random variable, as $XY = [(X+Y)^2 - (X^2+Y^2)]/2$, and a sum/function of random variables is a random variable, from the previous claims.&lt;/p&gt;
&lt;p&gt;We are now free to extend the claim that every continuous function of a random variable is a random variable, to piecewise continuity: &lt;strong&gt;every piecewise continuous function of a random variable is also a random variable&lt;/strong&gt;. If $f$ is piecewise continuous, then $f(X) = f_1(X) \mathbf{1}_{I_1} + f_2(X) \mathbf{1}_{I_2} + \ldots + f_n(X) \mathbf{1}_{I_n}$, where $f_j(X)$ are random variables as $f_j$ is continuous, and $I_j$ are disjoint intervals. From claim (3.6), $f(X)$ is a linear sum of random variables, and hence is also a random variable.&lt;/p&gt;
&lt;h2&gt;4. References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Rosenthal, Jeffrey S. &lt;em&gt;A First Look at Rigorous Probability Theory&lt;/em&gt;. World Scientific, 2006. &lt;em&gt;Open WorldCat&lt;/em&gt;, &lt;a href="http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5227675"&gt;http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5227675&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lebanon, Guy, editor. &lt;em&gt;Probability: The Analysis of Data ; Vol. 1.&lt;/em&gt; 2012. Available online at &lt;a href="http://theanalysisofdata.com/probability/0_2.html"&gt;http://theanalysisofdata.com/probability/0_2.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Math StackExchange, Wikipedia, etc etc :)&lt;/li&gt;
&lt;/ol&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>Function transforms (providing a broader picture of Laplace Transforms)</title><link href="https://aniruddha-deb.github.io/articles/2021/function-transforms.html" rel="alternate"></link><published>2021-05-12T00:00:00+05:30</published><updated>2021-05-12T00:00:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2021-05-12:/articles/2021/function-transforms.html</id><summary type="html">&lt;p&gt;I'll begin this article by brushing up a few definitions:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;function&lt;/strong&gt; is a &lt;strong&gt;mapping&lt;/strong&gt; between two sets: the domain D and the codomain C.&lt;/p&gt;
&lt;p&gt;&lt;img alt="function definition" src="/articles/2021/res/function.png"&gt;&lt;/p&gt;
&lt;p&gt;It's very important to note here that the function is &lt;strong&gt;the mapping itself&lt;/strong&gt;, and
not an element in the codomain or the domain. The function …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'll begin this article by brushing up a few definitions:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;function&lt;/strong&gt; is a &lt;strong&gt;mapping&lt;/strong&gt; between two sets: the domain D and the codomain C.&lt;/p&gt;
&lt;p&gt;&lt;img alt="function definition" src="/articles/2021/res/function.png"&gt;&lt;/p&gt;
&lt;p&gt;It's very important to note here that the function is &lt;strong&gt;the mapping itself&lt;/strong&gt;, and
not an element in the codomain or the domain. The function operates on an element
in the domain to give an element in the codomain. Formally, we would write this 
function as $f:D \to C$&lt;/p&gt;
&lt;p&gt;I'll also brush up on what a &lt;strong&gt;Vector Space&lt;/strong&gt; is: a vector space $V$ is a set of 
vectors over a given field $F$ on which two operations (functions) are defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$+:V \times V \to V$ - this is called Addition&lt;/li&gt;
&lt;li&gt;$\cdot : V \times F \to V$ - this is called Scalar Multiplication.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some other requisites of a vector space, such as the existence of 
an additive inverse and identity elements, but I won't get into the nitty-gritties
of those. Just keep in mind that &lt;strong&gt;addition and scalar multiplication over a 
vector space are closed.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Function Spaces&lt;/h2&gt;
&lt;p&gt;Suppose for a function $f:X \to V$, the codomain of a function is a vector space 
$V$ over $F$ ($X$ is the domain, which can be any set). Then, by definition, for any two elements $\vec{a}, \vec{b} \in V$ 
and a scalar $k \in F$, $\vec{a} + \vec{b} \in V$ and $k \cdot \vec{a} \in V$.&lt;/p&gt;
&lt;p&gt;Consider another function $g: X \to V$. Suppose now that for any $x \in X$, 
$f(x) = \vec{a}$ and $g(x) = \vec{b}$ for some $\vec{a}, \vec{b} \in V$. Therefore,
$\vec{a} + \vec{b} = f(x) + g(x) \in V$. We can define another function
$(f+g)$ such that $(f+g)(x) = f(x) + g(x)$. Note that &lt;strong&gt;this is valid
because V is a vector space, and because addition is closed under a vector space&lt;/strong&gt;.
The sum of two functions from $X \to V$ is also a function from $X \to V$!&lt;/p&gt;
&lt;p&gt;Let's try proving the same for scalar multiplication. For any $c \in F$, we have
$c\cdot\vec{a} = c\cdot f(x) \in V$. Define $(c \cdot f)$ such that 
$(c \cdot f)(x) = c \cdot f(x)$. This is also a function from $X \to V$.&lt;/p&gt;
&lt;p&gt;From the above two statements, we can see that the &lt;em&gt;maps themselves are linear&lt;/em&gt;:
that is, they can be added and scalarly multiplied, with the resulting map
preserving the domain and codomain. We can thus say that &lt;strong&gt;the functions thus
defined form a vector space themselves, which we call a function space&lt;/strong&gt;.
Note that there are a few more details involved, such as the existence
of an identity function and an additive inverse. Similar to the additive/multiplicative
proofs, these follow from the properties of the vector space. One interesting question to note
is &lt;em&gt;what if there is no function in the space that maps an element of $X$ to the identity element
$\vec{0}$ of $V$?&lt;/em&gt; Would the resulting function space still be a vector space?&lt;sup&gt;1&lt;/sup&gt; &lt;/p&gt;
&lt;h2&gt;Function transformations&lt;/h2&gt;
&lt;p&gt;If function spaces behave like vector spaces, this begs the question: what's the
equivalent of linear transformations for function spaces? The equivalent is called
a &lt;strong&gt;function transformation&lt;/strong&gt;, and it converts a function $f:X \to V$ to another 
function $T(f) = F:Y \to W$. Note that if $f$ lies in a function space $\mathcal{S}_f$, then 
$F$ need not lie in a function space! Consider the counterexample $P(f)$ on $f:X \to V$, which maps
all $x \in X$ to $1$. This is not a function space, as $F(x_1) + F(x_2) = 2 \not\in {1}$.
We would need to prove the linearity of $F$ independently of $f$ being a function 
space. Note also that this particular transform doesn't have an inverse. Transforms
that are linear in nature generally have an inverse transform associated with them.&lt;/p&gt;
&lt;p&gt;Some examples of function transformations include $Df = \frac{df}{dx}$ (the 
differential transform), as well as $I(f) = \int_a^b f dx$, the integral transform.
Note that these are both simple examples of transforms which preserve mapping, that is
if $f:\Bbb{R} \to \Bbb{R}$ and f is differentiable on $\Bbb{R}$, then $Df:\Bbb{R} \to \Bbb{R}$ as well. An example
of something that doesn't preserve mapping would be the parameterization 
transform for a multivariate function: if we have $f:\Bbb{R^3} \to \Bbb{R}$, then
the parameterization transform $P(f(x,y,z)) = F(x(t), y(t), z(t)) = F(t)$ gives
$F: \Bbb{R} \to \Bbb{R}$. &lt;/p&gt;
&lt;h2&gt;Integral transformations&lt;/h2&gt;
&lt;p&gt;Integral transformations are just one type of functional transform, but they're 
the most used type of transform due to their usefulness. A general integral
transform takes the form&lt;/p&gt;
&lt;p&gt;$$I(f) = \int_{x_1}^{x_2} K(s,x)f(x)dx$$&lt;/p&gt;
&lt;p&gt;$K(s,x)$ is called the &lt;em&gt;kernel&lt;/em&gt; of the integral transform, and the transform
has different intuitive meanings depending on the kernel and the limits used.
A full list of common kernels can be found at the &lt;a href="https://en.wikipedia.org/wiki/Integral_transform"&gt;Wikipedia page&lt;/a&gt; of
Integral transforms. A few key things to note however are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The integral transform exists only if the integral &lt;strong&gt;converges&lt;/strong&gt;. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;All integral transforms are linear&lt;/strong&gt;: this stems from the linearity property
   of the integral itself. Therefore, for a given function space, applying an 
   integral transform on every member of that function space would also give 
   us a function space.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;The Laplace transform&lt;/h2&gt;
&lt;p&gt;We finally come to the &lt;a href="https://en.wikipedia.org/wiki/Laplace_transform"&gt;Laplace transform&lt;/a&gt;: for the laplace transform, the kernel
is $K(s,x) = e^{-px}$ and the limits are $x_1 = 0$ and $x_2 = \infty$. 
Plugging this into the general integral transform gives us the laplace transform:&lt;/p&gt;
&lt;p&gt;$$\boxed{\mathcal{L}(f(x)) = F(p) = \int_0^\infty e^{-px}f(x)dx}$$&lt;/p&gt;
&lt;p&gt;Another bit of intuition regarding the laplace series comes from thinking about it
as a &lt;em&gt;continuous version of a power series&lt;/em&gt;. If we consider the power series&lt;/p&gt;
&lt;p&gt;$$F(x) = \sum_{n=0}^\infty f(n)x^n$$&lt;/p&gt;
&lt;p&gt;The continuous analogue of this power series is the integral&lt;/p&gt;
&lt;p&gt;$$F(x) = \int_0^\infty f(t)x^t\ dt = \int_0^\infty f(t)\left(e^{\ln x}\right)^t\ dt$$&lt;/p&gt;
&lt;p&gt;For this integral to converge, $\ln x &amp;lt; 0$. Let $p = -\ln x, p &amp;gt; 0$. The transform
then becomes&lt;/p&gt;
&lt;p&gt;$$F(p) = \int_0^\infty f(t)e^{-pt}\ dt$$&lt;/p&gt;
&lt;p&gt;Therefore, the laplace transform is like a continuous analogue of the power series.&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;p&gt;I haven't covered the several properties of the laplace transform: those specific
details can be found on the &lt;a href="https://en.wikipedia.org/wiki/Laplace_transform#Properties_and_theorems"&gt;Wikipedia page&lt;/a&gt;. 
This was mainly to give an overview of the mathematical backbone that goes into
transformations such as the laplace and fourier transforms.&lt;/p&gt;
&lt;p&gt;For further reading on function spaces, see &lt;a href="https://files.vipulnaik.com/exposition/functionspaces.pdf"&gt;Vipul Naik's notes&lt;/a&gt; and
&lt;a href="https://terrytao.files.wordpress.com/2008/03/function_spaces1.pdf"&gt;Terry Tao's notes&lt;/a&gt; on the same.
More on laplace transforms (including problems) are given in George Simmons' 
&lt;em&gt;Differential Equations with Applications and Historical Notes&lt;/em&gt;, chapter 9.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;No, it wouldn't. This is analogous to having $\vec{0}$ as an element
of a vector space: if a vector space doesn't have $\vec{0}$, then it's not a 
vector space as there is no element $k \in V$ such that for every element $v \in V$,
$k + v = v$.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>Understanding Jacobians</title><link href="https://aniruddha-deb.github.io/articles/2021/understanding-jacobians.html" rel="alternate"></link><published>2021-01-29T15:11:00+05:30</published><updated>2021-01-29T15:11:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2021-01-29:/articles/2021/understanding-jacobians.html</id><summary type="html">&lt;p&gt;$\newcommand{\pdv}[2]{\frac{\partial{#1}}{\partial{#2}}}$
$\newcommand{\ah}{\pmb{a} + \pmb{h}}$
$\newcommand{\a}{\pmb{a}}$
$\newcommand{\h}{\pmb{h}}$&lt;/p&gt;
&lt;h2&gt;The Jacobian Matrix&lt;/h2&gt;
&lt;p&gt;Consider a function that maps reals to reals, $f:\Bbb{R} \to \Bbb{R}$. The linear
approximation of this function is given by 
$$f(a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;$\newcommand{\pdv}[2]{\frac{\partial{#1}}{\partial{#2}}}$
$\newcommand{\ah}{\pmb{a} + \pmb{h}}$
$\newcommand{\a}{\pmb{a}}$
$\newcommand{\h}{\pmb{h}}$&lt;/p&gt;
&lt;h2&gt;The Jacobian Matrix&lt;/h2&gt;
&lt;p&gt;Consider a function that maps reals to reals, $f:\Bbb{R} \to \Bbb{R}$. The linear
approximation of this function is given by 
$$f(a+h) \approx f(a) + hf'(a)$$
This is pretty simple to do, and follows from taylor's expansion upto the first order.&lt;/p&gt;
&lt;p&gt;Let's try expanding this concept to vector spaces. For a function $f:\Bbb{R}^n \to \Bbb{R}$, 
it's linear approximation is given by 
$$f(\pmb{a} + \pmb{h}) \approx f(\pmb{a}) + \pmb{h}\cdot\nabla{f}(\a)$$
(bold type indicates vectors). This also follows from the taylor theorem for 
multivariable functions.&lt;/p&gt;
&lt;p&gt;What do we do when we have a function mapping vector spaces to vector spaces?
Consider the function $f:\Bbb{R}^n \to \Bbb{R}^m$. What would be the approximation
term here?
$$f(\pmb{a} + \pmb{h}) \approx f(\pmb{a})\ +\ ???$$&lt;/p&gt;
&lt;p&gt;Let's try to solve this by decomposing the function $f:\Bbb{R}^n \to \Bbb{R}^m$
into $f_i:\Bbb{R}^n \to \Bbb{R}$, $i = 1, 2, \cdots ,m$. For each $f_i$, we get
$$f_i(\pmb{a} + \pmb{h}) \approx f_i(\pmb{a}) + \pmb{h}\cdot\nabla{f_i}(\a)$$&lt;/p&gt;
&lt;p&gt;If we collect all these approximations into a vector by representing the term 
$\pmb{h}\cdot\nabla f_i(\a)$ as a matrix product, we get our approximation for $f$:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} f_1(\ah) \\ f_2(\ah) \\ \vdots \\ f_m(\ah) \end{bmatrix}
\approx 
\begin{bmatrix} f_1(\a) \\ f_2(\a) \\ \vdots \\ f_m(\a) \end{bmatrix} + 
\begin{bmatrix} \nabla^T f_1(\a) \\ \nabla^T f_2(\a) \\ \vdots \\ \nabla^T f_m(\a) \end{bmatrix}
\begin{bmatrix} h_1 \\ h_2 \\ \vdots \\ h_n \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;I've written the functions out in matrix form for clarity. $\nabla^T$ denotes 
the transpose of the gradient vector. In simpler terms, we can rewrite this as
$$f(\a + \h) \approx f(\a) + \pmb{\mathrm{J}}_f(\a)\h$$&lt;/p&gt;
&lt;p&gt;Here, $\pmb{\mathrm{J}}_f(\a)$ is called the &lt;strong&gt;Jacobian matrix&lt;/strong&gt;, and when expanded, it looks
something like this:&lt;/p&gt;
&lt;p&gt;$$\pmb{\mathrm{J}} = \begin{bmatrix} \pdv{f_1}{x_1} &amp;amp; \cdots &amp;amp; \pdv{f_1}{x_n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\ \pdv{f_m}{x_1} &amp;amp; \cdots &amp;amp; \pdv{f_m}{x_n}\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;The Jacobian Matrix thus, is an analog of the gradient vector for functions that
map vector spaces to vector spaces. Everything that we can do using gradients
can be done in a more general form using the Jacobian Matrix. Consider the 
condition for differentiability of a multivariate scalar function $f:\Bbb{R}^n \to \Bbb{R}$:
for $f$ to be differentiable at $\a$, we have the condition
$$\lim_{||\h|| \to 0} \frac{f(\ah) - f(\a) - \h \cdot \nabla f(\a)}{||\h||} = 0$$
For a function $f:\Bbb{R}^n \to \Bbb{R}^m$, the condition would be:
$$\lim_{||\h|| \to 0} \frac{||f(\ah) - f(\a) - \pmb{\mathrm{J}}_f(\a)\h||}{||\h||} = 0$$&lt;/p&gt;
&lt;p&gt;here, $||\cdot||$ is the euclidean norm of the vector.&lt;/p&gt;
&lt;h2&gt;Relating Jacobian Matrices and Transformation Matrices&lt;/h2&gt;
&lt;p&gt;If you notice, the Jacobian Matrix need not be square; for the special case 
$n = m$, that is for $f:\Bbb{R}^n \to \Bbb{R}^n$, the Jacobian matrix is square 
and acts as a linear transformation between the two n-dimensional vector spaces,
as shown below:
$$\begin{bmatrix} df_1 \\ \vdots \\ df_n \end{bmatrix} = 
\begin{bmatrix} \pdv{f_1}{x_1} &amp;amp; \cdots &amp;amp; \pdv{f_1}{x_n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\ \pdv{f_n}{x_1} &amp;amp; \cdots &amp;amp; \pdv{f_n}{x_n}\end{bmatrix} 
\begin{bmatrix} dx_1 \\ \vdots \\ dx_n \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;Therefore, this acts like a linear transformation between the infinitesimal 
elements in the space of $f$ and in the space of $x$. One more proof of scaling
is involved before the use of jacobian matrices in integrals becomes clear.&lt;/p&gt;
&lt;h2&gt;Scaling factor and the Jacobian Determinant&lt;/h2&gt;
&lt;p&gt;Recall that for any linear transformation, &lt;strong&gt;the determinant of the transformation
gives us the scaling factor&lt;/strong&gt;, that is the ratio of the change in 'volume' occupied
by the vector. This is also known as a dilation transformation (because just the 
size is involved, without worrying about orientation). &lt;/p&gt;
&lt;p&gt;I'll provide a proof for the statement highlighted above in $\Bbb{R}^3$, but 
extending it to $\Bbb{R}^n$ is easy enough (an exercise for the reader, as they 
say in math textbooks :)). Consider the unit cube centered at origin, ie 
having it's 3 vectors as $(1,0,0), (0,1,0)$ and $(0,0,1)$. On applying a transformation
to this cube, we get the vertices as the transformation matrix itself. Here's 
an image that speaks a thousand words (taken from &lt;a href="http://hopsblog-hop.blogspot.com/2017/02/"&gt;Hop's Blog&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img alt="scaling" src="res/cube_transform.jpg"&gt;&lt;/p&gt;
&lt;p&gt;In algebra terms, we get &lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} a &amp;amp; x &amp;amp; l \\ b &amp;amp; y &amp;amp; m \\ c &amp;amp; z &amp;amp; n \end{bmatrix} \begin{bmatrix} 1&amp;amp;0&amp;amp;0\\0&amp;amp;1&amp;amp;0\\0&amp;amp;0&amp;amp;1 \end{bmatrix} = 
\begin{bmatrix} a &amp;amp; x &amp;amp; l \\ b &amp;amp; y &amp;amp; m \\ c &amp;amp; z &amp;amp; n \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;The volume of a parallelepiped is given by the determinants of the 3 edge vectors,
and hence the volume of the transformed cube is 
$$V = \left| \begin{array}{c c c} a &amp;amp; x &amp;amp; l \\ b &amp;amp; y &amp;amp; m \\ c &amp;amp; z &amp;amp; n \end{array}\right|$$
Which is the determinant of the linear transform that we started with. $\blacksquare$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is the main principle that allows us to use the Jacobian in multiple 
integrals while changing variables&lt;/strong&gt;: it scales up or down the size of the 
area or volume element we are using proportionately to the change of variables.&lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://math.stackexchange.com/questions/14952/what-is-the-jacobian-matrix"&gt;What is the Jacobian Matrix&lt;/a&gt;, a good MSE thread
   on the Jacobian Matrix&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"&gt;Jacobian Matrix and Determinant Wikipedia page&lt;/a&gt;, of course&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-is-the-Jacobian-how-does-it-work-and-what-is-an-intuitive-explanation-of-the-Jacobian-and-a-change-of-basis#"&gt;A Quora question on Jacobian matrices&lt;/a&gt; with another very nice answer&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Personal comments:&lt;/h2&gt;
&lt;p&gt;I thoroughly enjoyed writing so much 'hard' mathematics on this blog after a 
long time (last proper math post was on 11th September of Last year, and other 
math notes in the interim were published on the &lt;a href="https://aniruddhadeb.com/MathNotes"&gt;MathNotes site&lt;/a&gt;).
A lot of calc textbooks don't go into detail on Jacobians, instead just using 
them like a gift of god that fell out of the sky. The bare minimum they would 
provide would be a diagram of domain transformation, and the cliched example of 
converting to polar integrals (the disc is transformed into a rectangle), but that
would still not make intuitive sense: &lt;strong&gt;why the determinant? And why this 
weird matrix?&lt;/strong&gt; were the questions that popped up in my head, and I hope I've done
justice to those questions in this article.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>Setting up MathNotes - an online math repository</title><link href="https://aniruddha-deb.github.io/articles/2020/mathnotes-launch.md.html" rel="alternate"></link><published>2020-10-16T12:54:00+05:30</published><updated>2020-10-16T12:54:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-10-16:/articles/2020/mathnotes-launch.md.html</id><summary type="html">&lt;p&gt;Just after 10th and as I was beginning my JEE Preparation, I felt the need for 
an extensible note-taking apparatus. Online notes would be too much trouble 
and would keep me hooked to the computer. Notebooks were also difficult, as 
I wanted my notes to be extensible; adding pages in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Just after 10th and as I was beginning my JEE Preparation, I felt the need for 
an extensible note-taking apparatus. Online notes would be too much trouble 
and would keep me hooked to the computer. Notebooks were also difficult, as 
I wanted my notes to be extensible; adding pages in the middle of a notebook 
is extremely difficult. I finally settled on using Binders; you can add/remove
pages quite easily and they're decently sized to accommodate all the notes you
could ever need.&lt;/p&gt;
&lt;p&gt;I took out two old binders; they had ~120 pages each and used one for math and 
one for physics. Soon enough, I had to get more pages and by the end of my 
JEE Journey, the binders had somewhere around 200 pages, each filled with notes&lt;/p&gt;
&lt;p&gt;&lt;img alt="Binders" src="res/mathnotes/binders.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Left one is maths, right one is physics&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="monotonicity" src="res/mathnotes/monotonicity.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="PnC" src="res/mathnotes/PnC.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="MI" src="res/mathnotes/MI.jpg"&gt;&lt;/p&gt;
&lt;p&gt;These tend to follow a very hierarchical format, and also tend to be extremely 
diverse with symbols and diagrams (I did all the notes in pencil, so that if 
any errors arise, I can change the content). &lt;/p&gt;
&lt;p&gt;I now needed a note-taking repository that had the same level of extensibility 
and one that I could use through college life. The computer was not a limitation 
now, and I was also decently skilled in LaTeX by now. I decided to make an 
online website using Sphinx and Read the docs, for keeping my math notes. &lt;/p&gt;
&lt;p&gt;I followed the great guide by Ryan Dale on &lt;a href="https://daler.github.io/sphinxdoc-test/includeme.html"&gt;settting up sphinx on GitHub Pages&lt;/a&gt;. 
I took a few liberties with the folder structure, but now setting up the repo
for contributors is as simple as cloning the repo and running the &lt;code&gt;install.sh&lt;/code&gt; 
script in it. 
The website is now live at &lt;a href="https://aniruddha-deb.github.io/MathNotes"&gt;https://aniruddha-deb.github.io/MathNotes&lt;/a&gt;, 
and I will be porting all my math and physics content over, and also creating 
notes in the future, for the math courses that I take.&lt;/p&gt;
&lt;p&gt;This means that this blog would feature fewer 'Math Notes' sections that I 
spoke about previously (link &lt;a href="https://aniruddha-deb.github.io/articles/2020/math-notes-1.html"&gt;here&lt;/a&gt;).
The math content will also go down, but I will continue to write about math 
stuff that made me go 'wow' (there are a few of those coming in the future, 
so stay tuned)&lt;/p&gt;
&lt;p&gt;If you would like to contribute to the math notes repository, feel free to 
reach out to me on my eMail, by clicking the mail icon in the sidebar.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="Programming"></category></entry><entry><title>A Good JEE Main (September) problem</title><link href="https://aniruddha-deb.github.io/articles/2020/good-jee-main-problem.html" rel="alternate"></link><published>2020-09-11T09:01:00+05:30</published><updated>2020-09-11T09:01:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-09-11:/articles/2020/good-jee-main-problem.html</id><summary type="html">&lt;p&gt;This beauty came in the 2nd September shift 2 paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let $A = \{ X = (x, y, z)^T : PX = 0 \text{ and } x^2+y^2+z^2=1 \}$, where 
$$P = \left[ \begin{array}{l l l}1&amp;amp;2&amp;amp;1 \\ -2&amp;amp;3&amp;amp;-4 \\ 1&amp;amp;9&amp;amp;-1 \end{array}\right]$$
then the …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;This beauty came in the 2nd September shift 2 paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let $A = \{ X = (x, y, z)^T : PX = 0 \text{ and } x^2+y^2+z^2=1 \}$, where 
$$P = \left[ \begin{array}{l l l}1&amp;amp;2&amp;amp;1 \\ -2&amp;amp;3&amp;amp;-4 \\ 1&amp;amp;9&amp;amp;-1 \end{array}\right]$$
then the set A:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;is a singleton&lt;/li&gt;
&lt;li&gt;is an empty set&lt;/li&gt;
&lt;li&gt;contains exactly two elements&lt;/li&gt;
&lt;li&gt;contains more than two elements&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a great problem, one that should have featured in advanced rather than 
main. It involves matrices and 3D Geometry and fits in to linear algebra more 
than any other topic.&lt;/p&gt;
&lt;p&gt;To solve this, notice that $A$ is a set of column vectors which lie on the intersection
of the three planes determined by $PX = 0$. The three planes are:
$$\begin{align}
x + 2y + z &amp;amp;= 0 \\
-2x + 3y - 4z &amp;amp;= 0 \\
x + 9y - z &amp;amp;= 0 \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;These three planes have a trivial solution $x = y = z = 0$. Let's see if they
have non-trivial solutions by obtaining $det(P)$:
$$\begin{align}
|P| &amp;amp;= \left| \begin{array}{l l l}1&amp;amp;2&amp;amp;1\\ -2&amp;amp;3&amp;amp;-4\\ 1&amp;amp;9&amp;amp;-1\end{array} \right| = -3 -8 -18 -3 +36 -4 \\
|P| &amp;amp;= 0 \
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, they have non-trivial solutions. All the planes are concurrent and form a 
line that passes through origin. Thus, if $S = \{ X = (\lambda_1x,\  \lambda_2y,\  \lambda_3z)^t : 
\lambda_1,\  \lambda_2,\  \lambda_3 \in R \}$ is the line formed by the planes, 
then S has only two elements for which the magnitude of $X$ is 1. These two elements
are the only elements of $A$, thus $A$ contains exactly two elements. $\blacksquare$&lt;/p&gt;
&lt;p&gt;A great problem, that required good thinking and visualization skills. The JEE
was a toss-up between great problems like this and stupid ones like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you spill a chemical toilet cleaning liquid on your hand, your first aid 
would be : &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vinegar&lt;/li&gt;
&lt;li&gt;aqueous $\ce{NH3}$&lt;/li&gt;
&lt;li&gt;aqueous $\ce{NaHCO3}$&lt;/li&gt;
&lt;li&gt;aqueous $\ce{NaOH}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here's to hoping the JEE advanced has better chemistry problems :)&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="JEE"></category></entry><entry><title>Math Notes: Lagrange interpolation</title><link href="https://aniruddha-deb.github.io/articles/2020/math-notes-1.html" rel="alternate"></link><published>2020-08-14T17:37:00+05:30</published><updated>2020-08-14T17:37:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-08-14:/articles/2020/math-notes-1.html</id><summary type="html">&lt;p&gt;This is a small set of posts that come into the category of "notes": 
self-explanations of concepts that I have recently picked up and found 
interesting.&lt;/p&gt;
&lt;h2&gt;Lagrange Interpolation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange Interpolation&lt;/a&gt; is a concept that allows us to find a polynomial of 
least degree passing through a given set of points …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a small set of posts that come into the category of "notes": 
self-explanations of concepts that I have recently picked up and found 
interesting.&lt;/p&gt;
&lt;h2&gt;Lagrange Interpolation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange Interpolation&lt;/a&gt; is a concept that allows us to find a polynomial of 
least degree passing through a given set of points. Specifically:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $S = {(x_i, y_i) : x_i, y_i \in R, 1 \lt i \le n}$, then the polynomial
of least degree passing through all the points in $S$ is given by
$$P(x) = \sum_{i=1}^n L_i(x) \cdot y_i$$
where $$L_i(x) = \frac{(x-x_1)(x-x_2)...(x-x_{i-1})(x-x_{i+1})...(x-x_n)}
{(x_i-x_1)(x_i-x_2)...(x_i-x_{i-1})(x_i-x_{i+1})...(x_i-x_n)}$$
or simply,
$$P(x) = \sum_{i=1}^n \frac{\prod_{r \ne i}(x-x_r)}{\prod_{r \ne i}{(x_i-x_r)}} \cdot y_i$$
note that the degree of polynomial $P &amp;lt; n$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here's an example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $P(x)$ is a function such that $P(-1) = -2,  P(0) = 2, P(1) = 0, P(2) = 2$, 
then find $P(x)$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So we have $S = {(-1,-2), (0,2), (1,0), (2,2)}$. Using lagrange interpolation 
gives us $P(x)$ as:
$$\begin{gather}
P(x) = \frac{(x-0)(x-1)(x-2)}{(-1-0)(-1-1)(-1-2)} \cdot -2 + \\
\frac{(x+1)(x-1)(x-2)}{(0+1)(0-1)(0-2)} \cdot 2 + \\
\frac{(x+1)(x-0)(x-2)}{(1+1)(1-0)(1-2)} \cdot 0 + \\
\frac{(x+1)(x-0)(x-1)}{(2+1)(2-0)(2-1)} \cdot -2
\end{gather}$$
or, once simplified,
$$P(x) = x^3 - 3x + 2$$&lt;/p&gt;
&lt;p&gt;This technique is also useful for other stuff, such as finding patterns in 
hard-to-decipher sequences:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Find the nth term of the sequence $$5, 65, 325, 1025, 2501 ...$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Letting $T(1) = 5, T(2) = 65$ and so on, lagrange interpolation gives us:
$$\begin{gather}
T(n) = \frac{5(x-2)(x-3)(x-4)(x-5)}{(-1)(-2)(-3)(-4)} + \\ 
\frac{65(x-1)(x-3)(x-4)(x-5)}{(1)(-1)(-2)(-3)} + \\
\frac{325(x-1)(x-2)(x-4)(x-5)}{(2)(1)(-1)(-2)} + \\
\frac{1025(x-1)(x-2)(x-3)(x-5)}{(3)(2)(1)(-1)} + \\
\frac{2501(x-1)(x-2)(x-3)(x-4)}{(4)(3)(2)(1)}
\end{gather}$$
simplified:
$$T(n) = 4n^4 + 1$$&lt;/p&gt;
&lt;p&gt;Of course, there are some problems that cannot be solved by this technique, 
namely problems where it is specified that the polynomial is of a higher order
than the number of points it passes through:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let $P(x)=x^4+ax^3+bx^2+cx+d$. If $P(1) = 1, P(2)=8, P(3)=27, P(4)=64$, then 
  the value of $P(5)$ is?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The lagrange interpolated polynomial for the above values is $P(x) = x^3$. However, 
this gives the wrong answer of $P(5) = 125$. The correct answer is $P(5) = 149$, 
because we have been asked to fit a polynomial of higher order in this equation.&lt;/p&gt;
&lt;p&gt;Also, this technique should only be used as a last resort for pattern finding. 
This is extremely computation heavy and is more of a bruteforce algorithm, that 
is better suited to computers rather than manual calculation. Most math softwares
have a lagrange interpolation function in them. Geogebra's is called &lt;code&gt;Polynomial()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As a bonus, here's the code I used to generate the above function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;

&lt;span class="n"&gt;pts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])])&lt;/span&gt;

&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;P(x) = &amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;nr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;dr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;
        &lt;span class="n"&gt;dr&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;nr&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(x+&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;nr&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(x-&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s2"&gt;frac{{&lt;/span&gt;&lt;span class="si"&gt;{2}{0}&lt;/span&gt;&lt;span class="s2"&gt;}}{{&lt;/span&gt;&lt;span class="si"&gt;{1}&lt;/span&gt;&lt;span class="s2"&gt;}} + &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="Math Notes"></category></entry><entry><title>Is the shortest solution to a problem the best?</title><link href="https://aniruddha-deb.github.io/articles/2020/shortest-solution-best-or-not.html" rel="alternate"></link><published>2020-07-26T21:00:00+05:30</published><updated>2020-07-26T21:00:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-07-26:/articles/2020/shortest-solution-best-or-not.html</id><summary type="html">&lt;p&gt;My old man always used to say that there are two ways to solve a problem: there's 
the horse way, and there's the donkey way. The donkey way involves tedious 
calculations, whereas the horse way 'cuts' through the problem. Take this problem
as an example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For each positive integer $n …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;My old man always used to say that there are two ways to solve a problem: there's 
the horse way, and there's the donkey way. The donkey way involves tedious 
calculations, whereas the horse way 'cuts' through the problem. Take this problem
as an example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For each positive integer $n$, let $$y_n = ((n+1)(n+2)...(n+n))^\frac1n$$ for
$x \in \mathbb{R}$. Let $[x]$ be the greatest integer less than or equal to $x$. 
If $\lim_{n \to \infty} y_n = L$, then the value of $[L]$ is ______ ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This problem is taken from the JEE(A) 2018 Paper 1 Math section. There are two 
ways of doing this: the first one is the naive, or donkey way:
$$\begin{gather}
L = \lim_{n \to \infty} \left((1+\frac1n)(1+\frac2n)....(1+\frac nn)\right)^\frac1n \\
\log L = \lim_{n \to \infty} \frac{1}{n} \sum_{r=0}^n \log\left(1+\frac rn\right) \\
\log L = \int_0^1 \log(1+x)dx \\
\log L = (1+x)\log(1+x) - x |_0^1 \\
\log L = 2\log2 - 1 \\
L = \frac{4}{e} \\
\boxed{[L] = 1} \\
\end{gather}$$&lt;/p&gt;
&lt;p&gt;It looks simple, because I've made it look simple. The integration by parts step
is time consuming, as well as the substitution and conversion into Riemann sum.
They're all simple steps, but quite lengthy when done in succession. This is 
how we are taught to approach most problems: do the steps to yield an answer. 
Here's an example of the 'horse' way:
$$\begin{gather}
(1+0)(1+0)(1+0)...(1+0) &amp;lt; (1+\frac1n)(1+\frac2n)...(1+\frac nn) &amp;lt; (1+1)(1+1)(1+1)...(1+1) \\
\lim_{n \to \infty} 1^\frac1n &amp;lt; \lim_{n \to \infty} \left((1+\frac1n)(1+\frac2n)....(1+\frac nn)\right)^\frac1n &amp;lt; \lim_{n \to \infty} 2^{n \cdot \frac1n} \\
1 &amp;lt; L &amp;lt; 2 \\
\boxed{[L] = 1}
\end{gather}$$&lt;/p&gt;
&lt;p&gt;And we're done! Simple application of sandwich theorem gives us the answer here
to the required accuracy. Looking at both techniques, the way of the horse 
always seems smarter and more elegant than the way of the donkey... or does it?
Here's an example of another problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If 
$$a = 1 + \frac{x^3}{3!} + \frac{x^6}{6!} + \frac{x^9}{9!} + ...$$
$$b = x + \frac{x^4}{4!} + \frac{x^7}{7!} + \frac{x^{10}}{10!} + ...$$
$$c = \frac{x^2}{2!} + \frac{x^5}{5!} + \frac{x^8}{8!} + \frac{x^{11}}{11!} + ...$$
the value of $a^3 + b^3 + c^3 - 3abc = ?$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This looks like an interesting problem. The horse way of doing this would be to
first make a (perfectly valid) assumption that the answer is independent of $x$, 
because the answer is a constant and if it was dependent on $x$, it would be 
of the form $f(x)$, which it clearly isn't. Since it is independent of $x$, 
let $x=0$, giving us $a=1, b=0, c=0$. Thus, $a^3 + b^3 + c^3 - 3abc = 1$, which
is the correct answer.&lt;/p&gt;
&lt;p&gt;This solution looks good, but it is hacky at best and incomplete at worst. 
Note that the problem setters have not defined the domain of $a,b,c$. An interesting
twist would have been if $x \in \mathbb{R}^+$, then this hack would have been 
&lt;a href="https://www.google.com/search?q=defenestrated"&gt;defenestrated&lt;/a&gt;. Here, the donkey method, though longer, 
is more elegant and meaningful than the horse method. To solve this the right 
way, recall the identity
$$a^3 + b^3 + c^3 - 3abc = (a+b+c)(a+b\omega+c\omega^2)(a+b\omega^2+c\omega)$$
where $\omega$ is the complex cube root of unity.
If we substitute $a,b,c$ in the RHS and use the taylor expansion of $e^x$, then 
we get:
$$e^x \cdot e^{x\omega} \cdot e^{x\omega^2}$$
this is the same as $e^{x(1+\omega+\omega^2)}$, and, as $1 + \omega + \omega^2 = 0$,
the final answer is 
$$e^{x \cdot 0} = \boxed{1}$$&lt;/p&gt;
&lt;p&gt;This is The Right Way&lt;sup&gt;TM&lt;/sup&gt; of solving the problem. Even though it is 
the lengthier, more mentally strenuous way, this is a case where the 'donkey'
method is more elegant than the 'horse' method.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>ISC 2020 Analysis: An application of Data Science and Statistics</title><link href="https://aniruddha-deb.github.io/articles/2020/isc-2020-analysis.html" rel="alternate"></link><published>2020-07-11T16:30:00+05:30</published><updated>2020-07-11T16:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-07-11:/articles/2020/isc-2020-analysis.html</id><summary type="html">&lt;p&gt;The ISC Exam results were released on 10th June, 3 PM IST. In this article, I'll be analyzing the results of the 117 students in the science stream of my school and showing how they performed. I'll also weave a story with the data and point out things that could …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The ISC Exam results were released on 10th June, 3 PM IST. In this article, I'll be analyzing the results of the 117 students in the science stream of my school and showing how they performed. I'll also weave a story with the data and point out things that could have been improved, which would be helpful for future students.&lt;/p&gt;
&lt;h3&gt;Index:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Elementary inferences: How have people fared overall?&lt;/li&gt;
&lt;li&gt;Frequency Distribution of Marks: How many people got the same marks in a subject?&lt;/li&gt;
&lt;li&gt;Quartiles: What score is comparatively a good score?&lt;/li&gt;
&lt;li&gt;Corellation Analysis: Is there a relation between marks scored in different subjects?&lt;/li&gt;
&lt;li&gt;Using other datapoints: Do girls perform better than boys? Are back-benchers doing poorly?&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Elementary Inferences&lt;/h3&gt;
&lt;p&gt;The Science Stream consists of 117 Students, with 80 boys and 37 girls, all of whom passed.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c|c|}
\hline
\text{Dataset} &amp;amp; \text{mean} &amp;amp; \text{median} &amp;amp; \text{mode} &amp;amp; \text{max} &amp;amp; \text{min} &amp;amp; \sigma \\
\hline
\text{Bo4%} &amp;amp; 83.11 &amp;amp; 84.75 &amp;amp; - &amp;amp; 97.75 &amp;amp; 57.5 &amp;amp; 8.86 \\
\text{Overall %} &amp;amp; 80.32 &amp;amp; 82.6 &amp;amp; 85.8 &amp;amp; 97.6 &amp;amp; 52.6 &amp;amp; 10.10 \\
\text{Boys Bo4%} &amp;amp; 83.70 &amp;amp; 85.5 &amp;amp; 80.25 &amp;amp; 97.75 &amp;amp; 57.5 &amp;amp; 8.62 \\
\text{Boys overall %} &amp;amp; 80.94 &amp;amp; 82.70 &amp;amp; - &amp;amp; 97.6 &amp;amp; 52.6 &amp;amp; 9.61 \\
\text{Girls Bo4 %} &amp;amp; 81.84 &amp;amp; 81.25 &amp;amp; - &amp;amp; 95.25 &amp;amp; 59.5 &amp;amp; 9.34 \\
\text{Girls overall %} &amp;amp; 78.98 &amp;amp; 80.5 &amp;amp; - &amp;amp; 94.8 &amp;amp; 54.6 &amp;amp; 11.10 \\
\hline
\end{array}$$
&lt;center&gt;&lt;sup&gt;(Bo4 = Best of 4, - = no mode exists, $\sigma$ = standard deviation)&lt;/sup&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The mean, median and mode lie in the $80-85 %$ range for almost all datasets. There is a standard deviation of 8.86 from the mean and the toppers lie $1.5\sigma$ right of mean ($&amp;gt;96.4\%$). Interestingly, the people who scored the least lie $3\sigma$ left of mean ($~56 \%$), which is more than the toppers lie right of the mean. The Boys and Girls datasets are compared in more detail in Section 5: Using other datapoints&lt;/p&gt;
&lt;p&gt;A similar raw data comparision can also be done with subjectwise marks:
$$\begin{array}{|c|c|}
\hline
\text{Dataset} &amp;amp; n &amp;amp; \text{mean} &amp;amp; \text{median} &amp;amp; \text{max} &amp;amp; \text{min} &amp;amp; \sigma \\
\hline
\text{English} &amp;amp; 117 &amp;amp; 87.54 &amp;amp; 88 &amp;amp; 96 &amp;amp; 70 &amp;amp; 4.46 \\
\text{Physics} &amp;amp; 117 &amp;amp; 78.75 &amp;amp; 81 &amp;amp; 100 &amp;amp; 45 &amp;amp; 13.94 \\
\text{Chemistry} &amp;amp; 116 &amp;amp; 72.49 &amp;amp; 71.50 &amp;amp; 98 &amp;amp; 42 &amp;amp; 13.65 \\
\text{Mathematics} &amp;amp; 107 &amp;amp; 73.98 &amp;amp; 77 &amp;amp; 100 &amp;amp; 16 &amp;amp; 19.32 \\
\text{Computer Science} &amp;amp; 80 &amp;amp; 88.85 &amp;amp; 91 &amp;amp; 100 &amp;amp; 54 &amp;amp; 8.20 \\
\text{Biology} &amp;amp; 28 &amp;amp; 89.14 &amp;amp; 90 &amp;amp; 97 &amp;amp; 75 &amp;amp; 5.35 \\
\text{Environmental Science} &amp;amp; 7 &amp;amp; 80.71 &amp;amp; 78 &amp;amp; 94 &amp;amp; 72 &amp;amp; 6.85 \\
\text{Art} &amp;amp; 4 &amp;amp; 88.25 &amp;amp; 88 &amp;amp; 91 &amp;amp; 86 &amp;amp; 2.22 \\
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;This is an interesting dataset; looking at it, a few stories are evident:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Low Standard deviation in English and Computer Science: This is an interesting theme. English, given it's size, has the lowest standard deviation of $4.46$. Computer Science has a remarkably high mean of $88.85$ and a median of $91$. &lt;del&gt;Most students in class do not go to external tuitions for these subjects&lt;/del&gt;(see edit). This means that the quality of education imparted in the class room is quite good and brings results for English and Computer Science.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;EDIT: Turns out that quite a few people who have taken Computer Science and scored above 90 went for tuitions. A Majority of students don't go to tuitions for English, though, and I believe that our school has an above-average showing in English this time. (reference)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;High standard deviation and low mean in Physics, Chemistry and Mathematics: Another very interesting (and scary) theme: these subjects make up the core of science and it is shocking to see students perform badly in these core subjects. Most students seek external help for these subjects, and in spite of the external help, are not able to perform well. The mean marks in Math are 74, while they should ideally be close to 90, due to the precise nature of the subject. The education system needs to have a serious rethink on where they are going wrong in imparting the knowledge of these core subjects to the students.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the following sections, I won't be focusing too much on Arts, EVS and Biology: Arts and EVS make up a very small dataset, whose results may not be reliable. The marks obtained in Biology are not representative of the students' potential: due to the exams being cancelled, the marks for Biology were derived via this averaging algorithm.&lt;/p&gt;
&lt;h3&gt;Frequency Distribution of Marks&lt;/h3&gt;
&lt;p&gt;I'll be plotting a few Histograms in this section and doing an overview of the 'spread' of marks across the spectrum. This helps visualize the data presented in the previous section. The data for most subjects fits into a bell curve, with the exception of Physics, Chemistry and Mathematics&lt;/p&gt;
&lt;p&gt;As a small primer: A histogram plots the frequency of a particular element in a data set. If 5 people scored 95 marks, then a bar 5 units tall would be placed at the 95th element. The graphs would make this clearer.&lt;/p&gt;
&lt;p&gt;Let's start with the small subjects: Arts, EVS and Biology&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/Art.png"&gt;
&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/EVS.png"&gt;
&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/Biology.png"&gt;&lt;/p&gt;
&lt;p&gt;There's not too much to see here, because of the sparsity of these graphs. Let's move on to English:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/English.png"&gt;&lt;/p&gt;
&lt;p&gt;The graph for English nicely highlights the bell curve distribution; a majority of people have scored 89, and almost all students have obtained marks in the $79-94$ range. Computer Science also shows a similar trend&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/CS.png"&gt;&lt;/p&gt;
&lt;p&gt;Apart from a few outliers on the lower end of the spectrum, most people have obtained above $85$ here, with 91 being the most common score. 17 people obtained a score of 95 and above. One person obtained a solid $100$. This is a good result in a not-so-easy subject.&lt;/p&gt;
&lt;p&gt;Moving on to Physics:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/Physics.png"&gt;&lt;/p&gt;
&lt;p&gt;The Median for Physics was much higher than that of Maths and Chemistry (In part maybe because the paper was easy). We see that around 29 people have scored less than 70 here. 92 is the mode for this dataset, which is a good score. Again, one person has scored $100$, which is not an impossible feat in any of the sciences. &lt;/p&gt;
&lt;p&gt;What immediately pops out in the Physics graph is that there is no well-defined 'peak' or 'cluster' in the data as there was for the previous subjects. This becomes even more apparent in Chemistry:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/Chemistry.png"&gt;&lt;/p&gt;
&lt;p&gt;Mark segmentation is heavily apparent in chemistry, with four clearly defined segments: those scoring above 95, those in the 80-95 range, those in the 60-80 range and those who obtained below 60. The segmentation explains the high standard deviation as well as the low mean (a large number of people are centered around 70 and 85). The number of people scoring above 90 drops heavily here, with only 10 people crossing the barrier. Amazingly, nobody obtained a 100 or a 99 in this paper, with 98 being the highest. &lt;/p&gt;
&lt;p&gt;Finally, Maths teaches us a lot of lessons:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/Mathematics.png"&gt;&lt;/p&gt;
&lt;p&gt;Marks in math resemble a bag of dropped marbles: there seems to be a slight pattern here, but one that is heavily tinged by apparent randomness. Heavy segmentation and sparse distribution show why the standard deviation is around 20 here. Inequality is also highly apparent here: those who do well at maths do exceptionally well (3 people scored a 100) and those who do poorly do extremely poorly. Part of this is due to the high-stakes nature of the math paper: the marking pattern has a tendency to be ruthless here, while the other part boils down to imparting math education to the people who don't have a knack for math. There are two lessons here: for the people who are yet to give their exams, Study math and Study math hard. For those who have already given their exams, If you have done poorly in Math, take comfort in the fact that you are not alone. Math (and Chemistry) was tricky this time and if you are taking a math-heavy field, you will have several opportunities in life to do better in this subject later.&lt;/p&gt;
&lt;p&gt;Finally, Here's the histograms for Best of 4 percentage and Overall percentage:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/Bo4Pct.png"&gt;
&lt;img alt="histogram" src="https://aniruddha-deb.github.io/articles/2020/res/results/histogram/TotPct.png"&gt;&lt;/p&gt;
&lt;h3&gt;Quartiles&lt;/h3&gt;
&lt;p&gt;What score is a good score? The same set of marks, when viewed by different people, can have different interpretations. While your parents may think you have done poorly, compared to your friends, you may have done pretty well! Quartiles take care of this: they convert your percentage to percentile (A concept we're all familiar with, after JEE Main) and tell you into which 'bucket' you fit:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;1st quartile or Lower quartile: below 25th percentile&lt;/li&gt;
&lt;li&gt;2nd quartile: between 25th-50th percentile&lt;/li&gt;
&lt;li&gt;3rd quartile: between 50th-75th percentile&lt;/li&gt;
&lt;li&gt;4th quartile or Upper quartile: above 75th percentile
This table shows the entry marks required to break into a particular quartile:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{array}{|c|c|}
\hline
\text{Dataset} &amp;amp; \text{Q1} &amp;amp; \text{Q2} &amp;amp; \text{Q3} &amp;amp; \text{Q4}\\
\hline
\text{Bo4%} &amp;amp; 0 &amp;amp; 78 &amp;amp; 84.75 &amp;amp; 89.75 \\
\text{Overall %} &amp;amp; 0 &amp;amp; 74.60 &amp;amp; 82.60 &amp;amp; 87.80 \\
\text{English} &amp;amp; 0 &amp;amp; 85 &amp;amp; 88 &amp;amp; 91 \\
\text{Physics} &amp;amp; 0 &amp;amp; 72 &amp;amp; 81 &amp;amp; 91 \\
\text{Chemistry} &amp;amp; 0 &amp;amp; 64 &amp;amp; 71.50 &amp;amp; 84.25 \\
\text{Mathematics} &amp;amp; 0 &amp;amp; 62.50 &amp;amp; 77 &amp;amp; 89.50 \\
\text{Computer Science} &amp;amp; 0 &amp;amp; 86 &amp;amp; 91 &amp;amp; 94 \\
\text{Biology} &amp;amp; 0 &amp;amp; 86 &amp;amp; 90 &amp;amp; 93.25 \\
\text{Environmental Science} &amp;amp; 0 &amp;amp; 78 &amp;amp; 78 &amp;amp; 82.50 \\
\text{Art} &amp;amp; 0 &amp;amp; 86.75 &amp;amp; 88 &amp;amp; 89.50 \\
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;As an example, if you obtained a best of 4 percentage of $85\%$, you would fit in the Third Quartile. This is because you broke through the first (0), second (78) and third (84.75) quartile entry limits but could not break into the fourth quartile (89.75). Hence, your quartile is detemined by the entry marks just below/equal to your own marks.&lt;/p&gt;
&lt;h3&gt;Corellation Analysis&lt;/h3&gt;
&lt;p&gt;Subjects are not disjoint sets: there is quite a bit of overlap between two subjects. As an example, Atomic Structure is a topic we study both in Physics and Chemistry. For a broader example, a firm understanding of Mathematics is required to do well in Physics and Physical Chemistry. &lt;/p&gt;
&lt;p&gt;&lt;img alt="On the other hand, physicists like to say physics is to math as sex is to masturbation." src="https://imgs.xkcd.com/comics/purity.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Obligatory XKCD :)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;In this analysis, I'll treat Mathematics as the 'glue' subject and analyze how Physics and Chemistry coreelate with Mathematics, as well as how Physics and Chemistry correlate with each other. &lt;/p&gt;
&lt;p&gt;First up, Mathematics-Physics and Mathematics-Chemistry corellations:&lt;/p&gt;
&lt;p&gt;&lt;img alt="correlation" src="https://aniruddha-deb.github.io/articles/2020/res/results/correlation/MP_correlation.png"&gt;
&lt;img alt="correlation" src="https://aniruddha-deb.github.io/articles/2020/res/results/correlation/MC_correlation.png"&gt;&lt;/p&gt;
&lt;p&gt;Notice that $r &amp;gt; 0.8$ for both of these correlations: this means that those who did well in math automatically did well in Physics and Chemistry, which reiterates a key point to upcoming students: learn to love Math. Also notice that $r_{MP} &amp;gt; r_{MC}$. This also validates the fact that Maths is more important for Physics than it is for Chemistry (which is kind of obvious, since Physical chemistry has around 30-40% weightage only). &lt;/p&gt;
&lt;p&gt;Physics-Chemistry yields an interesting graph:&lt;/p&gt;
&lt;p&gt;&lt;img alt="correlation" src="https://aniruddha-deb.github.io/articles/2020/res/results/correlation/PC_correlation.png"&gt;&lt;/p&gt;
&lt;p&gt;Amazingly, there is an even higher degree of correlation here than there is between Math-Physics and Math-Chemistry. This stumped me. If any of the readers have an idea as to why Physics and Chemistry are so interlinked (only the 12th syllabus), then feel free to drop a comment down below. The only theory I have for now is that there are 9 students who have not taken math but have taken Physics and Chemistry (or a similar permutation). These 9 extra data points are contributing the extra $0.04$ to the correlation coefficient.&lt;/p&gt;
&lt;h3&gt;Using Other Datapoints&lt;/h3&gt;
&lt;p&gt;Remember this table from the first section?&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c|c|}
\hline
\text{Dataset} &amp;amp; \text{mean} &amp;amp; \text{median} &amp;amp; \text{mode} &amp;amp; \text{max} &amp;amp; \text{min} &amp;amp; \sigma \\
\hline
\text{Boys Bo4%} &amp;amp; 83.70 &amp;amp; 85.5 &amp;amp; 80.25 &amp;amp; 97.75 &amp;amp; 57.5 &amp;amp; 8.62 \\
\text{Boys overall %} &amp;amp; 80.94 &amp;amp; 82.70 &amp;amp; - &amp;amp; 97.6 &amp;amp; 52.6 &amp;amp; 9.61 \\
\text{Girls Bo4 %} &amp;amp; 81.84 &amp;amp; 81.25 &amp;amp; - &amp;amp; 95.25 &amp;amp; 59.5 &amp;amp; 9.34 \\
\text{Girls overall %} &amp;amp; 78.98 &amp;amp; 80.5 &amp;amp; - &amp;amp; 94.8 &amp;amp; 54.6 &amp;amp; 11.10 \\
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;I'll be expanding on this to display separate statistics for Boys and Girls in all the subjects. One problem that exists is that the number of boys and girls is not the same: not normalizing the data beforehand would favour the boys in every case. In these graphs, both the boys and girls are normalized, such that each boy or girl represents a fraction of the total boys and girls (for boys this fraction is $\frac{1}{80}$ as there are 80 boys, and for girls this fraction is $\frac{1}{37}$, as there are 37 girls&lt;/p&gt;
&lt;p&gt;As we did previously, Let's start with Art, EVS and Biology, three subjects from which there are minimal inferences to be drawn:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/Art_BvG.png"&gt;
&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/EVS_BvG.png"&gt;
&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/Biology_BvG.png"&gt;&lt;/p&gt;
&lt;p&gt;Not too many inferences to be drawn here: EVS and Art have a very small sample size and Biology was calculated based on averages. Let's move on to English and Computer Science&lt;/p&gt;
&lt;p&gt;&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/English_BvG.png"&gt;
&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/Computers_BvG.png"&gt;&lt;/p&gt;
&lt;p&gt;Girls and Boys are pretty much on par with each other in Computer Science, whereas for English, Girls do better than Boys.&lt;/p&gt;
&lt;p&gt;The Sciences, however, have a different story to tell. Here's Physics:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/Physics_BvG.png"&gt;&lt;/p&gt;
&lt;p&gt;We clearly see that the boys are doing better than girls in Physics. A large number of girls have score in the $72-75$ mark range, as well as in the $46-53$ mark range. Girls lead boys in the $88-92$ mark range, however, the $95-100$ mark range is completely occupied by boys. Chemistry paints a similar picture:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/Chemistry_BvG.png"&gt;&lt;/p&gt;
&lt;p&gt;Again, the frequency of girls in the $&amp;lt;60$ mark range is high and the $95-100$ mark range is again occupied by boys. Girls do outperform boys in the $82-93$ mark range here.&lt;/p&gt;
&lt;p&gt;&lt;img alt="BvG" src="https://aniruddha-deb.github.io/articles/2020/res/results/BvG/Mathematics_BvG.png"&gt;&lt;/p&gt;
&lt;p&gt;Mathematics too is similar. There are no girls in the $97-100$ range and an abundance of them in the $&amp;lt;40$ mark range.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This analysis is by no means exhaustive: many other stories can also be told with exactly the same data set. However, the most important lesson that stands out is to Focus on Math. I cannot stress this enough. Math is required for Physics, Chemistry and to a small extent, Computer Science as well. Having a firm grounding in math is essential for all science students. Math is also a 'precise' subject, which means that the probability of scoring better marks in Math is higher than it is 'soft' subjects such as English.&lt;/p&gt;
&lt;p&gt;For the ones who have already given their papers, I hope that this analysis shows you how well you have done and also points out places where you could have improved. The real value of this analysis lies for the students who are yet to give their papers. I would have benefited immensely had such an analysis been available for me to read before my exams. I hope that upcoming students can use this wisdom to shape their own preparation strategy for the exams.&lt;/p&gt;
&lt;h3&gt;Behind the scenes&lt;/h3&gt;
&lt;p&gt;The data was analysed with Python. Matplotlib was used for drawing the beautiful graphs and numpy, along with python's inbuilt statistics library was used for doing the calculations. This is the first time I've ventured into data science, and I'm really enjoying it :)  R was a candidate for doing most of this processing (I really like R's ggplot2 library: it's built on solid concepts), but I already had my python development environment set up on my machine and wanted to do this in a language I'm comfortable with.&lt;/p&gt;
&lt;p&gt;Further reading (for those interested in Data Science):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Towards Data Science - Amazing medium blog on Data Science&lt;/li&gt;
&lt;li&gt;R Project - R, a language for statistical computing and graphics&lt;/li&gt;
&lt;li&gt;Statistics How to - For absolute beginners in statistics&lt;/li&gt;
&lt;/ul&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="ISC"></category><category term="Data Science"></category></entry><entry><title>Solving the African Integral (from YG file cover)</title><link href="https://aniruddha-deb.github.io/articles/2020/african-integral.html" rel="alternate"></link><published>2020-07-09T20:30:00+05:30</published><updated>2020-07-09T20:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-07-09:/articles/2020/african-integral.html</id><summary type="html">&lt;p&gt;A long time ago, I gave my friend (Let's call him C) an integral to solve. He came back to me a few days later and the conversation went something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;C:&lt;/strong&gt; Debu, I couldn't solve that African integral you gave me.&lt;br&gt;
&lt;strong&gt;Me:&lt;/strong&gt; Ok, but why are you calling …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;A long time ago, I gave my friend (Let's call him C) an integral to solve. He came back to me a few days later and the conversation went something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;C:&lt;/strong&gt; Debu, I couldn't solve that African integral you gave me.&lt;br&gt;
&lt;strong&gt;Me:&lt;/strong&gt; Ok, but why are you calling it African?&lt;br&gt;
&lt;strong&gt;C:&lt;/strong&gt; Because Africa is a really big continent, y'know?&lt;br&gt;
&lt;strong&gt;Me:&lt;/strong&gt; Ok, but isn't Asia bigger?&lt;br&gt;
&lt;strong&gt;C:&lt;/strong&gt; Debu, please stop making me feel dumb&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The integral in question appears prominently on the YG file cover as well, and is also featured in Advanced Problems in Mathematics for JEE(A) by Vikas Gupta (Question 24 chapter 7, for those following along)&lt;/p&gt;
&lt;p&gt;&lt;img alt="YG File cover" src="https://aniruddha-deb.github.io/articles/2020/res/yg_file.jpg"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Evaluate $$\int e^{x\sin x + \cos x} \left( \frac{x^4\cos^3x - x\sin x + \cos x}{x^2 \cos^2x}\right) dx$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This looks fearsome, but we can solve it easily by extending the trick $\int e^x (f(x) + f'(x)) dx = e^xf(x) + c$ just a little bit. Consider the function $$e^{g(x)}f(x)$$ Differentiating said function, we see that $$\frac{d}{dx} (e^{g(x)}f(x)) = e^{g(x)}(f'(x) + g'(x)f(x))$$ Rearranging the terms and integrating on both sides: $$e^{g(x)}f(x) = \int e^{g(x)}(f'(x) + g'(x)f(x)) dx$$ For the given problem, $g(x) = x\sin x + \cos x \implies g'(x) = x\cos x$. Therefore, the answer will be something of the form $$I = e^{x\sin x + \cos x} f(x) + c$$ The problem now becomes finding $f(x)$, which we can do through some rearrangement. Rewriting the problem to fit in $g'(x)$, the problem becomes: $$I = \int e^{x\sin x + \cos x} \left( x\cos x \cdot x + \frac{\sec x}{x^2} - \frac{\tan x \sec x}{x}\right) dx$$ We can see that the first term in the bracket implies that $f(x) = x$, but $f'(x) \ne \frac{\sec x}{x^2} - \frac{\tan x \sec x}{x}$. It's easy to see that the last two terms have some kind of pattern going on, as $\frac{d}{dx}\sec x = \sec x \tan x$ and $\frac{d}{dx} \frac{1}{x} = -\frac{1}{x^2}$. The last two terms can be rewritten as $\frac{d}{dx} \left( \frac{\sec x}{x}\right)$. Go through the above steps once again if you're unsure of what is happening, because this is important. The integral can then be rewritten as: 
$$I = \int e^{x\sin x + \cos x} \left( x\cos x \cdot x - \frac{d}{dx} \left( \frac{\sec x}{x}\right) \right) dx$$&lt;/p&gt;
&lt;p&gt;We're finally getting somewhere! We had initially assumed that $f(x) = x$, but now, seeing the differential term in the bracket, let's assume 
$$f(x) = x - \frac{\sec x}{x} \implies f'(x) = 1 - \frac{d}{dx} \left( \frac{\sec x}{x}\right)$$&lt;/p&gt;
&lt;p&gt;Also, $g'(x)f(x)$ then becomes 
$$g'(x)f(x) = x\cos x \left(x - \frac{\sec x}{x}\right) = x\cos x \cdot x - 1$$&lt;/p&gt;
&lt;p&gt;Adding them together, we get
$$g'(x)f(x) + f'(x) = x\cos x \cdot x - \frac{d}{dx} \left( \frac{\sec x}{x}\right)$$&lt;/p&gt;
&lt;p&gt;Which is the term in the brackets in the integral. Thus, $f(x) = x - \frac{\sec x}{x}$. We have now solved our integral, and the answer is
$$\boxed{I =  e^{x\sin x + \cos x} \left( x - \frac{\sec x}{x}\right) + c}$$&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>A Compilation of hard limits</title><link href="https://aniruddha-deb.github.io/articles/2020/hard-limits.html" rel="alternate"></link><published>2020-06-05T11:30:00+05:30</published><updated>2020-06-05T11:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-06-05:/articles/2020/hard-limits.html</id><summary type="html">&lt;p&gt;This list consists of the limits that I found most challenging.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;$$\lim_{n \to \infty} \left( \frac{n!}{n^n} \right) ^\frac 1n$$&lt;/li&gt;
&lt;li&gt;$$\lim_{x \to 0} \left( \frac{1}{\ln(x + \sqrt{x^2+1})} - \frac 1{\ln(x+1)} \right)$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \frac{n + n^2 …&lt;/li&gt;&lt;/ol&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;This list consists of the limits that I found most challenging.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;$$\lim_{n \to \infty} \left( \frac{n!}{n^n} \right) ^\frac 1n$$&lt;/li&gt;
&lt;li&gt;$$\lim_{x \to 0} \left( \frac{1}{\ln(x + \sqrt{x^2+1})} - \frac 1{\ln(x+1)} \right)$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \frac{n + n^2 + n^3 + ... + n^n}{1^n + 2^n + 3^n + ... + n^n}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \left( \frac{n^n(x+n)\left(x+\frac n2\right)...\left(x+\frac nn\right)}{n!(x^2+n^2)\left(x^2+\frac {n^2}{4}\right)...\left( x^2 + \frac{n^2}{n^2}\right)}\right)^{\frac x n}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{x \to 0} \left( 1^{\sin^{-2}x} + 2^{\sin^{-2}x} + ... + n^{\sin^{-2}x}\right)^{\sin^2 x}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \sqrt[\leftroot{-2}\uproot{2}n+1]{(n+1)!}-\sqrt[\leftroot{-2}\uproot{2}n]{n!}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \left( e - \left( 1 + \frac1n \right) ^n \right) ^\frac 1n$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \underbrace{\sin(\sin(\sin(...\sin(a)..)))}_\text{n times}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \sqrt{1 + 2\sqrt{1 + 3\sqrt{1 + 4\sqrt{ ... 1 + (n-1)\sqrt{1+n}}}}}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{x \to \infty} \binom{x}{n} \left(\frac{a}{x}\right)^n \left( 1 - \frac{a}{x}\right)^{x-n}$$&lt;/li&gt;
&lt;li&gt;$$\lim_{n \to \infty} \ \frac{2^{n+1}}{n+1} \ \left| 3\sum_{k=1}^n (-1)^k\frac{k}{2^k} + \frac 23\right|$$&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;$e$&lt;/li&gt;
&lt;li&gt;$-\frac 12$&lt;/li&gt;
&lt;li&gt;$1 - \frac 1e$&lt;/li&gt;
&lt;li&gt;$\exp\left( \int_0^x \ln \left( \frac{t+1}{t^2+1}\right)dt\right)$ (from JEE(A) 2016)&lt;/li&gt;
&lt;li&gt;$n$&lt;/li&gt;
&lt;li&gt;$e^{-1}$&lt;/li&gt;
&lt;li&gt;$1$&lt;/li&gt;
&lt;li&gt;$0$&lt;/li&gt;
&lt;li&gt;$3$ (Ramanujan found this one :)&lt;/li&gt;
&lt;li&gt;$\frac{a^n e^a}{n!}$&lt;/li&gt;
&lt;li&gt;$2$&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>Limit involving higher order infinitesimals</title><link href="https://aniruddha-deb.github.io/articles/2020/limit-higher-order-infinitesimals.html" rel="alternate"></link><published>2020-06-05T07:30:00+05:30</published><updated>2020-06-05T07:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-06-05:/articles/2020/limit-higher-order-infinitesimals.html</id><summary type="html">&lt;p&gt;Simple limit problems consist of the form $\lim_{x \to 0}\frac{O_1(x)O_2(x)..}{O_a(x)O_b(x)..}$, such as $\lim_{x \to 0} \frac{\sin 3x \tan 2x \tan^{-1} 5x}{x^2 \ln(1+x)}$. Here, the infinitesimals are well defined and cancel out easily. Some …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Simple limit problems consist of the form $\lim_{x \to 0}\frac{O_1(x)O_2(x)..}{O_a(x)O_b(x)..}$, such as $\lim_{x \to 0} \frac{\sin 3x \tan 2x \tan^{-1} 5x}{x^2 \ln(1+x)}$. Here, the infinitesimals are well defined and cancel out easily. Some trickier limit problems involve the difference of two infinitesimals. A good example is $\lim_{t \to 0} \frac{\sin t - \tan t}{t^3}$. These kind of problems require expansions to solve them cleanly. L'Hopital rule is tedious as it will need to be repeated atleast thrice for this given limit, which involves infinitesimals of order 3. To illustrate how to solve these problems, I'll give a trickier example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Evaluate the limit $$\lim_{n \to \infty} \left( e - \left( 1 + \frac1n \right) ^n \right) ^\frac 1n$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's easy to assume that the limit $\lim_{n \to \infty} \left( 1 + \frac1n \right)^n$ evaluates to $e$, which means that the expression inside the outer brackets would tend to zero, giving the answer as zero. However, all is not as it seems. This is a good example of difference between infinitesimals yielding a higher order infinitesimal. To start with, consider the function
$$f(n) =  \lim_{n \to \infty} \left( 1 + \frac1n \right)^n$$
Taking a logarithm both sides and using a taylor series expansion for the log on the right side gives
$$\begin{gather}
\log f(n) = \lim_{n \to \infty} n \left( \frac1n - \frac 1{2n^2} + O\left(\frac 1 {n^3}\right) \right) \\
f(n) = \lim_{n \to \infty}e^{1 - \frac 1{2n} + O\left(\frac 1 {n^2}\right)}
\end{gather}$$&lt;/p&gt;
&lt;p&gt;If we substitute $f(n)$ in the limit now, we can see that
$$L = \lim_{n \to \infty} \left( e \left( 1 - e^{-\frac 1{2n} + O\left(\frac 1 {n^2}\right)}\right) \right) ^\frac 1n$$&lt;/p&gt;
&lt;p&gt;Taking a logarithm both sides gives us
$$\log L = \lim_{n \to \infty} \underbrace{\frac{1}{n}}_{f_1} + \underbrace{\frac{ \log\left( 1 - e^{-\frac 1{2n} + O\left(\frac 1 {n^2}\right)}\right)}{n}}_{f_2}$$&lt;/p&gt;
&lt;p&gt;$f_1$ is trivially $0$. $f_2$ can be evaluated using L'Hopital rule, which gives
$$f_2 = \lim_{n \to \infty} \frac{\frac{1}{2n^2} + ... }{1 - e^{-\frac 1{2n} + O\left(\frac 1 {n^2}\right)}}$$&lt;/p&gt;
&lt;p&gt;This evaluates to $0$ as the numerator is a higher order infinitesimal than the denominator. Thus, we have
$$\log L = 0$$&lt;/p&gt;
&lt;p&gt;Taking an exponent both sides gives us
$$\boxed{L = 1}$$&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>COVID-19 USA Analysis: effects of the lack of lockdown</title><link href="https://aniruddha-deb.github.io/articles/2020/covid-19-usa.html" rel="alternate"></link><published>2020-05-29T20:00:00+05:30</published><updated>2020-05-29T20:00:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-05-29:/articles/2020/covid-19-usa.html</id><summary type="html">&lt;p&gt;The USA currently stands at 1.76 million COVID-19 cases. That's more than the next 5 nations combined. A large number of these cases are due to government inaction against the virus. The lack of a concerted lockdown across the country is also to blame. Here's a graph showing the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The USA currently stands at 1.76 million COVID-19 cases. That's more than the next 5 nations combined. A large number of these cases are due to government inaction against the virus. The lack of a concerted lockdown across the country is also to blame. Here's a graph showing the daily number of cases of COVID-19 in the USA.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="Daily covid cases" src="https://aniruddha-deb.github.io/articles/2020/res/covid-19-usa/covid-cases-daily.png"&gt;&lt;/center&gt;&lt;br&gt;
&lt;center&gt;&lt;sup&gt;Source: Google&lt;/sup&gt;&lt;/center&gt;&lt;br&gt;
Notice something strange? After reaching saturation, the curve shows a weird oscillatory behaviour. Is there a pattern to this? Let's overlay some week markers on the data to get a better feel of it.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="covid weekly" src="https://aniruddha-deb.github.io/articles/2020/res/covid-19-usa/covid-weekly.png"&gt;&lt;/center&gt;&lt;br&gt;
&lt;center&gt;&lt;sup&gt;Source: Our World in data*&lt;/sup&gt;&lt;/center&gt;&lt;br&gt;
The pattern is much more obvious now. It is clearly visible that reported cases are at their lowest on the first few days of the week and they are at their highest during the weekends. Summing up the data proves this (quite dramatically)&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="covid sum" src="https://aniruddha-deb.github.io/articles/2020/res/covid-19-usa/covid-sum.png"&gt;&lt;/center&gt;&lt;br&gt;
&lt;center&gt;&lt;sup&gt;Sum of cases/day from 1 Mar to today&lt;/sup&gt;&lt;/center&gt;&lt;br&gt;
This data now allows us to paint a picture. The mean time of incubation of Coronavirus is 5.1 days&lt;sup&gt;[1]&lt;/sup&gt;. One inference is that more people are going out on Sunday night and getting tested on Friday morning. This is not very probable as most of the hospitality business (malls, hotels etc) is shut. The more likely reason is that &lt;em&gt;people are going to work on a Monday and Tuesday, displaying symptoms in a week and then getting tested at the end of the week&lt;/em&gt;. &lt;strong&gt;This situation would not arise if a nationwide lockdown was implemented&lt;/strong&gt;. This is the key reason behind the large number of cases and also behind the recurring patterns of occurence of new cases.&lt;/p&gt;
&lt;p&gt;The analysis also presents several lessons for nations in the nascent phase of the virus: India has lifted it's lockdown just as the number of cases are peaking. While isolation of zones where infected patients have been found continues, urging people to return to work in these times may present the same pattern as shown above.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;* Our World in Data recorded a spike of ~48000 cases on April 26. This seemed suspicious, which is why that data point was substituted by the Google/Wikipedia data for the same date.&lt;/p&gt;
&lt;p&gt;[1]: Lauer, Stephen A., et al. “The Incubation Period of Coronavirus Disease 2019 (COVID-19) From Publicly Reported Confirmed Cases: Estimation and Application.” Annals of Internal Medicine, vol. 172, no. 9, Mar. 2020, pp. 577–82. acpjournals.org (Atypon), doi:&lt;a href="https://doi.org/10.7326/M20-0504"&gt;10.7326/M20-0504&lt;/a&gt;.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="COVID 19"></category></entry><entry><title>COVID 19 Regression analysis Update</title><link href="https://aniruddha-deb.github.io/articles/2020/covid-regression-analysis-update.html" rel="alternate"></link><published>2020-05-19T22:30:00+05:30</published><updated>2020-05-19T22:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-05-19:/articles/2020/covid-regression-analysis-update.html</id><summary type="html">&lt;p&gt;The Previous COVID regression analysis was fairly accurate. However, the opening of lockdown offset the statistics a bit and now there are more number of projected cases. Here is a recomputation of the statistics, which projects an average of 172,000 cases by June 1 and 520,000 cases overall …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Previous COVID regression analysis was fairly accurate. However, the opening of lockdown offset the statistics a bit and now there are more number of projected cases. Here is a recomputation of the statistics, which projects an average of 172,000 cases by June 1 and 520,000 cases overall by August end.&lt;/p&gt;
&lt;p&gt;&lt;img alt="covid static" src="https://aniruddha-deb.github.io/articles/2020/res/covid_update/covid_static.png"&gt;&lt;br&gt;
&lt;sup&gt;Y-axis: number of new cases per day. X-axis: number of days since first case.&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Here's an animated view:&lt;/p&gt;
&lt;p&gt;&lt;img alt="covid dynamic" src="https://aniruddha-deb.github.io/articles/2020/res/covid_update/covid_dynamic.gif"&gt;&lt;/p&gt;
&lt;p&gt;The relevant files can be found in &lt;a href="https://github.com/Aniruddha-Deb/COVID_19_regression"&gt;this GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;UPDATE: As of May 30 9:30 pm, The recorded cases are ~174,000. I was assuming a ~10% error margin on this prediction, but I ended up being quite close to the number. The final number of cases are expected to be ~181,000 by 31st May night, resulting in an error margin of 4.1%.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;P.S: Feel free to clone and tweak the data while citing this as the original source. I do not assume responsibility in the event that this data proves to be wrong or otherwise.&lt;/p&gt;
&lt;p&gt;P.P.S: GeoGebra 6 sucks big time! GeoGebra 5 da real MVP.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="COVID 19"></category></entry><entry><title>No square ends in 3</title><link href="https://aniruddha-deb.github.io/articles/2020/no-square-ends-in-3.html" rel="alternate"></link><published>2020-05-15T11:30:00+05:30</published><updated>2020-05-15T11:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-05-15:/articles/2020/no-square-ends-in-3.html</id><summary type="html">&lt;p&gt;This is an interesting number theory fact that seems strange when taken at face value. Here's a small proof of it:&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is an interesting number theory fact that seems strange when taken at face value. Here's a small proof of it:
$$\begin{array}{|c|c|}
\hline
\text{last digit of number} &amp;amp; \text{last digit of square} \\
\hline
0 &amp;amp; 0 \\
1 &amp;amp; 1 \\
2 &amp;amp; 4 \\
3 &amp;amp; 9 \\
4 &amp;amp; 6 \\
5 &amp;amp; 5 \\
6 &amp;amp; 6 \\
7 &amp;amp; 9 \\
8 &amp;amp; 4 \\
9 &amp;amp; 1 \\
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;It's easy to see that no square is ending in 3 (or 2, 7 or 8 for that matter). What are the implications of this? Try the following sum:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How many solutions of $x$ exist such that $$\sum_{i=1}^x i! = n^2, \space n \in N$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In simpler terms, what summations of the factorial give us a perfect square? We can work out the first few by hand
$$\begin{gather}
x = 1 \implies S_1 = 1! = 1\\
x = 2 \implies S_2 = S_1 + 2! = 3 \\
x = 3 \implies S_3 = S_2 + 3! = 9 \\
x = 4 \implies S_4 = S_3 + 4! = 33
\end{gather}$$&lt;/p&gt;
&lt;p&gt;For $5!$ and above, the last number is always 0. When we use the above recurrence relation for $x \ge 5$, the last digit will always be 3, and since no square ends in 3, there are only two values of $x$ for which the given equation holds, which are $1$ and $3$.  $\blacksquare$&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category></entry><entry><title>COVID 19 regression analysis</title><link href="https://aniruddha-deb.github.io/articles/2020/covid-regression-analysis.html" rel="alternate"></link><published>2020-05-06T21:30:00+05:30</published><updated>2020-05-06T21:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-05-06:/articles/2020/covid-regression-analysis.html</id><summary type="html">&lt;p&gt;&lt;img alt="regression" src="https://aniruddha-deb.github.io/articles/2020/res/covid-regression-analysis.png"&gt;
This is a regression analysis attempt for the COVID-19 spread data. The graphs represent total cases per day. The Orange graph is USA, the smaller graph on the left is China and the graph on the right is India. A Standard gaussian curve of the form $Ae^{-b(x-c)^2 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="regression" src="https://aniruddha-deb.github.io/articles/2020/res/covid-regression-analysis.png"&gt;
This is a regression analysis attempt for the COVID-19 spread data. The graphs represent total cases per day. The Orange graph is USA, the smaller graph on the left is China and the graph on the right is India. A Standard gaussian curve of the form $Ae^{-b(x-c)^2}$ is fitted on the data manually (They don't fit the USA data very well though)&lt;/p&gt;
&lt;p&gt;The y-axis scale is 5000 cases/large division and 1000 cases/small division. Currently, 50 represents today's date for India i.e. May 6. Looking at a predicted worst case curve, we see it peaking somewhere around May 20 ($\pm$ 5 days). It should go down post that point. The total number of cases is the integral under the curve. For a gaussian distribution, it comes out to be $a\sqrt{\frac \pi b}$. Taking half of this (before the peak), we end up with $\approx 130,000$ cases. India can expect to see this number $\pm$ 10% of cases by May end and a overall of $\approx 200,000 \pm 10%$ cases by June end, worst case. &lt;/p&gt;
&lt;p&gt;Let's hope I'm wrong here and we can flatten the curve sooner than expected.&lt;/p&gt;</content><category term="Mathematics"></category><category term="Mathematics"></category><category term="COVID 19"></category></entry><entry><title>Roots of $f(f(..f(x)..))$, where $ f(x) = ax^2 + bx + c $, are symmetric about $ \frac{-b}{2a} $</title><link href="https://aniruddha-deb.github.io/articles/2020/roots-of-fffx.html" rel="alternate"></link><published>2020-04-30T20:30:00+05:30</published><updated>2020-04-30T20:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-04-30:/articles/2020/roots-of-fffx.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Define $ f(x) = ax^2 + bx + c , a,b,c \in \mathbb{R}$ and $ f^n(x) = f(f^{n-1}(x)), n&amp;gt;1 $. Prove that the real roots of $ f^n(x) $ are symmetric about the vertical line passing through vertex i.e. $ x = \frac{-b}{2a} $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This seems like …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Define $ f(x) = ax^2 + bx + c , a,b,c \in \mathbb{R}$ and $ f^n(x) = f(f^{n-1}(x)), n&amp;gt;1 $. Prove that the real roots of $ f^n(x) $ are symmetric about the vertical line passing through vertex i.e. $ x = \frac{-b}{2a} $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This seems like a complicated problem. Let's start by first making intuitional sense of this problem. We can say that $ f^n(x) $ will be a polynomial of the order $ 2^n $ and will have as many roots. Also, all roots of a quadratic of the form $ ax^2 + bx + c , a,b \in \mathbb{R} $ can be expressed as $ \frac{-b}{2a} \pm k$, where $k$ is some constant, which can be real or complex. To find all the real roots of $ f^n(x) $, we have to solve the set of equations $ f^{n-1}(x) = \alpha_i \backepsilon \alpha_i $ is a root of $ f^{n-1}(x) $.&lt;/p&gt;
&lt;p&gt;This problem can be recursively broken down until we arrive at finding the roots of $ f^2(x) $, which are the roots of $ f(x) = \alpha_1 = \frac{-b}{2a} + k $ and $ f(x) = \alpha_2 = \frac{-b}{2a} - k $. Since these two quadratics also assume the form in the paragraph discussed above, their roots can be written as $ \frac{-b}{2a} \pm u $ and $ \frac{-b}{2a} \pm v $. Note that if the equation has complex roots, then the values of u and v would be imaginary. This does not matter, as any further equations solved upon them will continue to have their roots expressed as $ \frac{-b}{2a} \pm k $. Moving to solve $ f^3(x) $ requires us to solve the equations $f(x) = \frac{-b}{2a} \pm u $ and $f(x) = \frac{-b}{2a} \pm v $. If we continue to do this, we can notice that all the $ 2^n $ roots of $ ax^2 + bx + c $ can be expressed in the same form $ \frac{-b}{2a} \pm v $, as the value of the first two coefficients of the quadratic(s) to be solved remains the same. Thus, all the real roots are symmetric about the said vertical line and all imaginary roots have their real part equal to $ \frac{-b}{2a} $. $ \blacksquare $&lt;/p&gt;
&lt;p&gt;A graphical proof for this is as follows: consider any arbitary quadratic with two real roots (this can also be proven with a quadratic with no real roots, but that requires mapping the complex plane on the z-axis, which I don't want to do).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Quadratic f(x)" src="https://aniruddha-deb.github.io/articles/2020/res/roots_1.png"&gt;&lt;/p&gt;
&lt;p&gt;Here the roots are A and B. Reflecting them onto the Y-axis and drawing two parallel lines through them gives us the following graph:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Quadratic with roots on y-axis" src="https://aniruddha-deb.github.io/articles/2020/res/roots_2.png"&gt;&lt;/p&gt;
&lt;p&gt;The two lines represent the roots and the points of intersection of the parabola with the line (That's C,D,E and F) represent the roots of the equation $f^2(x)$. This can be verified by plotting $ f(f(x)) $ in the same graph&lt;/p&gt;
&lt;p&gt;&lt;img alt="Quadratic f(f(x))" src="https://aniruddha-deb.github.io/articles/2020/res/roots_3.png"&gt;&lt;/p&gt;
&lt;p&gt;This provides a more intuitive approach to the above proof. As is clearly visible, $ f^2(x) $ has all it's roots symmetrical. This procedure can be repeated $n$ times, giving us the requisite proof.&lt;/p&gt;
&lt;p&gt;Both of my proofs are not very mathematically rigorous. A rigorous proof would involve induction using the principles I showed here. That exercise is left to the readers (mainly because my induction &amp;amp; formal proof-writing skills are horrible :P). &lt;/p&gt;
&lt;p&gt;An application of this problem can be found in the &lt;a href="https://jason-shi-f9dm.squarespace.com/s/2019Algebra_A.pdf"&gt;PUMAC 2019 Algebra A paper, Q3&lt;/a&gt;. This is a rather nice problem, which involves the sum of the roots of $ f^n(x) $. Since all the roots are symmetric, you would end up with $ 2^n $ times the abscissa of the vertex of said quadratic.&lt;/p&gt;</content><category term="Mathematics"></category></entry><entry><title>An integral involving ζ(2) (And Euler's first proof for the Basel Problem)</title><link href="https://aniruddha-deb.github.io/articles/2020/zeta-2-integral.html" rel="alternate"></link><published>2020-04-29T22:30:00+05:30</published><updated>2020-04-29T22:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2020-04-29:/articles/2020/zeta-2-integral.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Evaluate the integral $$\int_0^1 \frac{\log(x)}{x-1}dx$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are a few methods of doing this: the first one uses the taylor series expansion of \( \log(1-x) \):
$$I = -\int_0^1 \frac{\log(1-x)}{x}dx$$ 
$$I = \int_0^1 \frac 1x \left( x + \frac{x^2}{2} + \frac{x …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Evaluate the integral $$\int_0^1 \frac{\log(x)}{x-1}dx$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are a few methods of doing this: the first one uses the taylor series expansion of \( \log(1-x) \):
$$I = -\int_0^1 \frac{\log(1-x)}{x}dx$$ 
$$I = \int_0^1 \frac 1x \left( x + \frac{x^2}{2} + \frac{x^3}{3} + ... \right) $$&lt;/p&gt;
&lt;p&gt;This expression nicely integrates to give
$$I = 1 + \frac{1}{2^2} + \frac{1}{3^2} + ... = \boxed{\frac{\pi^2}{6}}$$&lt;/p&gt;
&lt;p&gt;The last step here involves the summation \( \zeta(2) = \sum_{k=1}^{\infty} \frac 1 {k^2} = \frac{\pi^2}{6}\). This summation is known as the &lt;a href="https://en.wikipedia.org/wiki/Basel_problem"&gt;Basel Problem&lt;/a&gt; (more on this later). For now, we'll use this as a fact and move forward.&lt;/p&gt;
&lt;p&gt;Another solution that does not involve the taylor series requires some substitutions and limit evaluation.&lt;/p&gt;
&lt;p&gt;$$\begin{gather}
I = \int_0^1\frac{\log x}{x-1} dx= -\int_0^1\log x(1+x+x^2+...)dx \\
= -\int_0^1 \log x \sum_{r=0}^\infty x^r dx \\
= -\sum_{r=0}^\infty\int_0^1x^r \log x dx 
\end{gather}$$&lt;/p&gt;
&lt;p&gt;let \(\log x = t \implies dx = e^t dt\):
$$\begin{gather}I = -\sum_{r=0}^\infty \int_{-\infty}^0te^{(r+1)t}dt \\
=-\sum_{r=0}^\infty \left[\frac{te^{(r+1)t}}{(r+1)} - \frac{e^{(r+1)t}}{(r+1)^2}\right]_{-\infty}^0\\
= \sum_{r=0}^\infty \left[\frac{1}{(r+1)^2} + \lim_{t \to -\infty} \frac{(t(r+1)-1)e^{(r+1)t}}{(r+1)^2} \right]\\
= \sum_{r=0}^\infty \left[\frac{1}{(r+1)^2} - \lim_{t \to \infty} \frac{(t(r+1)+1)}{e^{(r+1)t}(r+1)^2} \right] \\
= \sum_{r=0}^\infty \left[\frac{1}{(r+1)^2} - \lim_{t \to \infty} \frac{r+1}{e^{(r+1)t}(r+1)^3} \right] \\
= \sum_{r=0}^\infty \frac{1}{(r+1)^2}\\
= \sum_{k=1}^\infty \frac{1}{k^2}\\
\boxed{I = \frac{\pi^2}{6}}
\end{gather}$$&lt;/p&gt;
&lt;p&gt;This solution also involves the same summation. There is, however, a solution that does not involve the summation: The general form of this integral is known as &lt;a href="https://en.wikipedia.org/wiki/Spence%27s_function"&gt;Spence's Function&lt;/a&gt; or Dilogarithm. It's a transcendental function which has the values \( Li_2(0) = 1 \) and \( Li_2(1) = \zeta(2) = \frac{\pi^2}{6} \). If you know these values beforehand, you can cut out the lengthy calculation.&lt;/p&gt;
&lt;p&gt;Back to the Basel Problem, and a bit of a primer first. The &lt;a href="https://en.wikipedia.org/wiki/Riemann_zeta_function"&gt;Riemann-Zeta&lt;/a&gt; function is a mathematical function that is defined as \( \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} \). Thus, a convenient way of referring to the sum \( \sum_{k=1}^{\infty} \frac{1}{k^2} \) would be \( \zeta(2) \), which I will use hereon. The quest for the value of \( \zeta(2) \) far predates the Riemann-zeta function (and Bernhard Riemann himself for that matter). This problem was first posed by Pietro Mengoli, an Italian mathematician who had proved that the harmonic series \( \left( \sum_{n=1}^{\infty} \frac 1n \right) \) diverges and also found the value of the sum \( \sum_{r=1}^\infty (-1)^{r-1} \frac 1r \) to be \( \log(2) \). The problem is called the Basel problem as Basel in Switzerland is the hometown of Euler, who solved the problem and the Bernoulli family, who unsuccessfully attacked the problem.&lt;/p&gt;
&lt;p&gt;The story goes that the first proof Euler gave was not accepted as it involved an unproven factorization of \( \sin(x) \). Euler being Euler, gave a second proof and a third proof as well, which were accepted. The initial proof is as follows: From the taylor series expansion of \( \sin(x) \), we have
$$\sin(x) = x + \frac{x^3}{3!} + \frac{x^5}{5!} + ... $$&lt;/p&gt;
&lt;p&gt;\( \sin(x) \) can also be written as a function of it's roots, according to the &lt;a href="https://en.wikipedia.org/wiki/Weierstrass_factorization_theorem"&gt;Weierstrass Factorization Theorem&lt;/a&gt;. Note that this theorem was published a 100 years after Euler's proof, which is why Euler's proof was initially not accepted. Euler factorized \( \sin(x) \) as follows:
$$\sin(x) = x(1 - \frac x\pi)(1 + \frac x\pi)(1 - \frac{x}{2\pi})(1+\frac{x}{2\pi})...$$ $$\sin(x) = x(1-\frac{x^2}{\pi^2})(1-\frac{x^2}{4\pi^2})... \tag{1}$$&lt;/p&gt;
&lt;p&gt;Why did Euler choose to factorize \( \sin(x) \) as such? He could also have chosen the function \(x(x-\pi)(x+\pi)(x-2\pi)(x+2\pi)...\). A simple trick I used to explain this is that the function chosen by Euler is bounded and also converges to a common limit. Dividing both sides by \( x \) and evaluating the limit \(lim_{x \to 0} \frac{\sin(x)}{x}\) in equation \( (1) \), we see that both sides converge to 1.&lt;/p&gt;
&lt;p&gt;Back to the proof, since we have two equivalent representations of \( \sin(x) \), all we have to do is compare the coefficients of \( x^3 \) in both of them to find the value of \( \zeta(2) \)
$$\frac{1}{3!} = \frac{1}{\pi^2} \sum_{k=1}^{\infty} \frac{1}{k^2}$$
$$\boxed{\sum_{k=1}^{\infty} \frac{1}{k^2} = \frac{\pi^2}{6}}$$&lt;/p&gt;
&lt;p&gt;This completes the proof. A really beautiful and simple proof, this was the one of the two proofs (Along with &lt;a href="https://en.wikipedia.org/wiki/Basel_problem#Cauchy's_proof"&gt;Cauchy's proof&lt;/a&gt;) that I was able to understand. For more proofs involving higher order mathematics, you can check out the links &lt;a href="http://math.cmu.edu/~bwsulliv/basel-problem.pdf"&gt;here&lt;/a&gt; and &lt;a href="https://math.stackexchange.com/questions/8337/different-methods-to-compute-sum-limits-k-1-infty-frac1k2-basel-pro"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="Mathematics"></category></entry><entry><title>ICSE Mathematics - Last 23 years analysis and 2018 forecast</title><link href="https://aniruddha-deb.github.io/articles/2018/icse-mathematics-analysis.html" rel="alternate"></link><published>2018-01-09T10:30:00+05:30</published><updated>2018-01-09T10:30:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddha-deb.github.io,2018-01-09:/articles/2018/icse-mathematics-analysis.html</id><summary type="html">&lt;p&gt;A speculative format for the 2018 ICSE Mathematics paper, created by analysis of the mathematics question papers of the past 23 years. &lt;img alt="Analysis results" src="/articles/2018/res/icse_math_analysis.png"&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Analysis results" src="https://aniruddha-deb.github.io/articles/2018/res/icse_math_analysis.png"&gt;&lt;/p&gt;
&lt;p&gt;A speculative format for the 2018 ICSE Mathematics paper, created by analysis of the mathematics question papers of the past 23 years. The boards are fast approaching and hope this helps out those in need.&lt;/p&gt;
&lt;p&gt;Best of luck for the exams!&lt;/p&gt;
&lt;p&gt;- Deb&lt;/p&gt;
&lt;p&gt;EDIT: As of 16th June, 2018, I cleared the 2018 ICSE Mathematics paper with a perfect 100. The above statistical analysis truly paid off as most of the questions followed the same pattern :D&lt;/p&gt;</content><category term="Mathematics"></category></entry></feed>