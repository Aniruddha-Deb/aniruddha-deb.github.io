<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Aniruddha Deb - Machine Learning</title><link href="https://aniruddhadeb.com/" rel="alternate"></link><link href="https://aniruddhadeb.com/feeds/machine-learning.atom.xml" rel="self"></link><id>https://aniruddhadeb.com/</id><updated>2023-03-08T00:12:00+05:30</updated><entry><title>The Properly Illustrated Transformer</title><link href="https://aniruddhadeb.com/articles/2023/properly-illustrated-transformer.html" rel="alternate"></link><published>2023-03-08T00:12:00+05:30</published><updated>2023-03-08T00:12:00+05:30</updated><author><name>Aniruddha Deb</name></author><id>tag:aniruddhadeb.com,2023-03-08:/articles/2023/properly-illustrated-transformer.html</id><content type="html">&lt;p&gt;&lt;img alt="That's a f@!#ton of dropouts" src="/articles/2023/res/transformer_no_ws.svg"&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The title is (obviously) a shout out to &lt;a href="https://jalammar.github.io/illustrated-transformer/"&gt;Jay Alammar's Blog&lt;/a&gt;,
which is probably the best source to learn about transformers, followed by
&lt;a href="http://nlp.seas.harvard.edu/annotated-transformer/"&gt;The Annotated Transformer&lt;/a&gt;.
Both of them are unclear about the nuances though (where the dropouts are mostly),
     and so I made this to clear that up.&lt;/p&gt;</content><category term="Machine Learning"></category><category term="Machine Learning"></category><category term="Transformers"></category></entry></feed>