<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Aniruddha Deb</title>
    <link>https://aniruddhadeb.com/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Aniruddha Deb</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 04 Jun 2023 01:00:00 +0000</lastBuildDate><atom:link href="https://aniruddhadeb.com/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My (updated) DL workflow for 2023</title>
      <link>https://aniruddhadeb.com/articles/2023/updated-dl-workflow-2023/</link>
      <pubDate>Sun, 04 Jun 2023 01:00:00 +0000</pubDate>
      
      <guid>https://aniruddhadeb.com/articles/2023/updated-dl-workflow-2023/</guid>
      <description>Remember last semester, where I said this?
That went pretty well. Much better than I expected it to go :) I also used the HPC DL environment a lot, across 4 courses that I did (COL380, COL772, COL775 and COD310). It was so used that at a point I had four different GPU&amp;rsquo;s allocated just for running tasks for these different courses.
A lot could be done better, though. Since I have my summer ahead of me, I decided to clean house for the next semester</description>
    </item>
    
    <item>
      <title>Batch Normalization</title>
      <link>https://aniruddhadeb.com/articles/2023/batch-normalization/</link>
      <pubDate>Thu, 16 Feb 2023 16:00:00 +0000</pubDate>
      
      <guid>https://aniruddhadeb.com/articles/2023/batch-normalization/</guid>
      <description>$$ \require{physics} \newcommand{B}{\mathcal{B}}$$
Batch Normalization was proposed by Ioffe and Szegedy in 2015, and it spawned several normalization techniques that are used in SOTA models today (layer norm, weight norm, etc). Batch normalization normalizes the output of each layer based on the mean and variance of the examples in the current batch. Formally, if $\B = {x_1, \ldots, x_m}$ is our batch, then batch norm does the following transformation:
$$\begin{align} \mu_{\B} &amp;amp;= \frac{1}{m} \sum_i x_i \\ \sigma_{\B}^2 &amp;amp;= \left( \frac{1}{m} \sum_i x_i^2 \right) - \mu_{\B}^2 \\ \hat{x}_i &amp;amp;= \frac{x_i - \mu_{\B}}{\sqrt{\sigma^2_{\B} + \epsilon}} \\ y_i &amp;amp;= \gamma \hat{x}_i + \beta \end{align}$$</description>
    </item>
    
    <item>
      <title>L2 regularization intuition</title>
      <link>https://aniruddhadeb.com/articles/2023/l2-regularization/</link>
      <pubDate>Sun, 22 Jan 2023 10:15:00 +0000</pubDate>
      
      <guid>https://aniruddhadeb.com/articles/2023/l2-regularization/</guid>
      <description>A nice intuition for L2 regularization comes from having a prior on the distribution of parameters: the prior assumes that the parameters are close to zero. Let&amp;rsquo;s assume that the prior is $\mathcal{N}(0, \Sigma)$. The MAP estimate of the parameters would then be
$$\begin{align} \theta_{\text{MAP}} &amp;amp;= \text{argmax}_{\theta} ; P(\theta | D) \\ &amp;amp;= \text{argmax}_{\theta} ; P(D | \theta) P(\theta) \\ &amp;amp;= \text{argmax}_{\theta} ; \log P(D | \theta) + \log P(\theta) \end{align}$$</description>
    </item>
    
    <item>
      <title>Optimizers, Part 1</title>
      <link>https://aniruddhadeb.com/articles/2023/optimizers-1/</link>
      <pubDate>Mon, 02 Jan 2023 12:25:00 +0000</pubDate>
      
      <guid>https://aniruddhadeb.com/articles/2023/optimizers-1/</guid>
      <description>Happy New Year! This is going to was supposed to be a long one, so sit back and grab a chocolate (and preferably view this on your laptop)
Some optimization algorithms. Click on a colour in the legend to hide/show it
Table of Contents Introduction Do Solutions Even Exist? How this guide is structured Gradient Descent Optimizers Stochastic Gradient Descent SGD with Momentum SGD with Nesterov Momentum Putting it all together References and Footnotes Introduction Most supervised learning tasks involve optimizing a loss function, in order to fit the model to the given training data.</description>
    </item>
    
    <item>
      <title>My DL workflow for 2023</title>
      <link>https://aniruddhadeb.com/articles/2022/dl-workflow-2023/</link>
      <pubDate>Thu, 29 Dec 2022 23:40:00 +0000</pubDate>
      
      <guid>https://aniruddhadeb.com/articles/2022/dl-workflow-2023/</guid>
      <description>I&amp;rsquo;ve kind of zeroed down on Deep Learning at this point, and putting my money where my mouth is, will be taking both COL772 (Natural Language Processing) and COL775 (Deep Learning) next semester.
Along with Operating Systems, Parallel Programming and Theory of Computation.
Why a workflow? I&amp;rsquo;ll need to train a lot of models, and while Kaggle and Colab are great at this, 30 hours a week won&amp;rsquo;t cut it. So I&amp;rsquo;ll need to move to using the IITD HPC to train some (most?</description>
    </item>
    
  </channel>
</rss>
