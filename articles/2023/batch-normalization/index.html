<!DOCTYPE html>
<html class="not-ready lg:text-base" lang="en-us" style="--bg: #faf8f1">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<title>Batch Normalization - Aniruddha Deb</title>
<meta name="theme-color"/>
<meta content="$$ \require{physics} \newcommand{B}{\mathcal{B}}$$
Batch Normalization was proposed by Ioffe and Szegedy in 2015, and it spawned several normalization techniques that are used in SOTA models today (layer norm, weight norm, etc). Batch normalization normalizes the output of each layer based on the mean and variance of the examples in the current batch. Formally, if $\B = {x_1, \ldots, x_m}$ is our batch, then batch norm does the following transformation:
$$\begin{align} \mu_{\B} &amp;= \frac{1}{m} \sum_i x_i \\ \sigma_{\B}^2 &amp;= \left( \frac{1}{m} \sum_i x_i^2 \right) - \mu_{\B}^2 \\ \hat{x}_i &amp;= \frac{x_i - \mu_{\B}}{\sqrt{\sigma^2_{\B} + \epsilon}} \\ y_i &amp;= \gamma \hat{x}_i + \beta \end{align}$$" name="description"/>
<meta content="Aniruddha Deb" name="author"/>
<link as="style" href="https://aniruddhadeb.com/main.min.css" rel="preload stylesheet"/>
<link as="image" href="https://aniruddhadeb.com/theme.svg" rel="preload"/>
<link as="image" href="/pic.png" rel="preload"/>
<link as="image" href="https://aniruddhadeb.com/twitter.svg" rel="preload"/>
<link as="image" href="https://aniruddhadeb.com/github.svg" rel="preload"/>
<link as="image" href="https://aniruddhadeb.com/rss.svg" rel="preload"/>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processEscapes: true
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js" type="text/javascript">
</script>
<link href="https://aniruddhadeb.com/favicon/favicon.ico" rel="icon"/>
<link href="https://aniruddhadeb.com/favicon/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://aniruddhadeb.com/favicon/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://aniruddhadeb.com/favicon/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://aniruddhadeb.com/favicon/site.webmanifest" rel="manifest"/>
<meta content="Hugo 0.115.1" name="generator"/>
<meta content="Batch Normalization" itemprop="name"/>
<meta content="$$ \require{physics} \newcommand{B}{\mathcal{B}}$$
Batch Normalization was proposed by Ioffe and Szegedy in 2015, and it spawned several normalization techniques that are used in SOTA models today (layer norm, weight norm, etc). Batch normalization normalizes the output of each layer based on the mean and variance of the examples in the current batch. Formally, if $\B = {x_1, \ldots, x_m}$ is our batch, then batch norm does the following transformation:
$$\begin{align} \mu_{\B} &amp;= \frac{1}{m} \sum_i x_i \\ \sigma_{\B}^2 &amp;= \left( \frac{1}{m} \sum_i x_i^2 \right) - \mu_{\B}^2 \\ \hat{x}_i &amp;= \frac{x_i - \mu_{\B}}{\sqrt{\sigma^2_{\B} + \epsilon}} \\ y_i &amp;= \gamma \hat{x}_i + \beta \end{align}$$" itemprop="description"/><meta content="2023-02-16T16:00:00+00:00" itemprop="datePublished"/>
<meta content="2023-02-16T16:00:00+00:00" itemprop="dateModified"/>
<meta content="809" itemprop="wordCount"/>
<meta content="Mathematics,Machine Learning,Deep Learning," itemprop="keywords"/>
<meta content="summary" name="twitter:card"/>
<meta content="Batch Normalization" name="twitter:title"/>
<meta content="$$ \require{physics} \newcommand{B}{\mathcal{B}}$$
Batch Normalization was proposed by Ioffe and Szegedy in 2015, and it spawned several normalization techniques that are used in SOTA models today (layer norm, weight norm, etc). Batch normalization normalizes the output of each layer based on the mean and variance of the examples in the current batch. Formally, if $\B = {x_1, \ldots, x_m}$ is our batch, then batch norm does the following transformation:
$$\begin{align} \mu_{\B} &amp;= \frac{1}{m} \sum_i x_i \\ \sigma_{\B}^2 &amp;= \left( \frac{1}{m} \sum_i x_i^2 \right) - \mu_{\B}^2 \\ \hat{x}_i &amp;= \frac{x_i - \mu_{\B}}{\sqrt{\sigma^2_{\B} + \epsilon}} \\ y_i &amp;= \gamma \hat{x}_i + \beta \end{align}$$" name="twitter:description"/>
</head>
<body class="text-black duration-200 ease-out dark:text-white">
<header class="mx-auto flex h-[4.5rem] max-w-4xl px-8 lg:justify-center">
<div class="relative z-50 mr-auto flex items-center">
<a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href="https://aniruddhadeb.com">Aniruddha Deb</a>
<div aria-label="Dark" class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]" role="button"></div>
</div>
<div aria-label="Menu" class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role="button"></div>
<script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>
<div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none">
<nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
<a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href="/articles/">Blog</a>
<a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href="/publications/">Publications</a>
<a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href="/files/cv.pdf">CV</a>
</nav>
<nav class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6">
<a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" href="https://twitter.com/hairband_dude" rel="me" style="--url: url(./twitter.svg)" target="_blank">
        twitter
      </a>
<a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" href="https://github.com/Aniruddha-Deb" rel="me" style="--url: url(./github.svg)" target="_blank">
        github
      </a>
<a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" href="https://aniruddhadeb.com/index.xml" rel="alternate" style="--url: url(./rss.svg)" target="_blank">
        rss
      </a>
</nav>
</div>
</header>
<main class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-4xl px-8 pb-16 pt-12 dark:prose-invert">
<article>
<header class="mb-16">
<h1 class="!my-0 pb-2.5">Batch Normalization</h1>
<div class="text-sm antialiased opacity-60">
<time>Feb 16, 2023</time>
</div>
</header>
<section><p>$$
\require{physics}
\newcommand{B}{\mathcal{B}}$$</p>
<p>Batch Normalization was proposed by <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf">Ioffe and Szegedy</a>
in 2015, and it spawned several normalization techniques that are used in SOTA
models today (layer norm, weight norm, etc). Batch normalization normalizes
the output of each layer based on the mean and variance of the examples in the
current batch. Formally, if $\B = {x_1, \ldots, x_m}$ is our batch, then
batch norm does the following transformation:</p>
<p>$$\begin{align}
\mu_{\B} &amp;= \frac{1}{m} \sum_i x_i \\
\sigma_{\B}^2 &amp;= \left( \frac{1}{m} \sum_i x_i^2 \right) - \mu_{\B}^2 \\
\hat{x}_i &amp;= \frac{x_i - \mu_{\B}}{\sqrt{\sigma^2_{\B} + \epsilon}} \\
y_i &amp;= \gamma \hat{x}_i + \beta
\end{align}$$</p>
<h3 id="why">Why?</h3>
<p>When networks are trained, the partial derivative of a parameter obtained
during backprop gives us the gradient <em>assuming all other parameters remain
constant</em>. This is not true, as the parameters simultaneously change. This
causes the input distribution to a layer in a deep neural network to change
significantly from update to update, and this is called <em>Internal Covariate
Shift</em>.</p>
<p>Mathematically, if we have a simple linear model $y = w_l w_{l-1} \ldots w_1 x$, and
we update each parameter via the partial derivative of that parameter, then we
get $\hat{y} = (w_l - \epsilon g_l) \ldots (w_1 - \epsilon g_1) x$. Note how even the
second order terms ($\epsilon^2 (\prod_{i=l}^3 w_i)g_2 g_1$) may become very large,
making learning harder.</p>
<p>Batch norm aims to reduce internal covariate shift by normalizing the layers.
This centers them and reduces variance due to noisy parameter updates, while not
taking any information away (the model is still free to learn whatever mean
and variance it chooses to learn by updating $\gamma$ and $\beta$.)</p>
<h3 id="backprop">Backprop</h3>
<p>Before manually doing the backprop, a computation graph really helps here.</p>
<p>
<img alt="comp_graph" src="/articles/2023/res/batch_norm_graph.jpg"/>
</p>
<p>This is a small computation graph that uses just three examples, but it’s
sufficient to show which variables are dependent on each other, and how. This
allows us to go ahead and obtain all the partial derivatives via chain rule:</p>
<p>$$\begin{align}
\pdv{l}{\gamma} &amp;= \sum_i \pdv{l}{y_i} \hat{x}_i \\
\pdv{l}{\beta} &amp;= \sum_i \pdv{l}{y_i} \\
\pdv{l}{\hat{x}_i} &amp;= \pdv{l}{y_i} \gamma \\
\end{align}$$</p>
<p>The next three are a bit tricky. To check the derivation, just follow the edges
of the computation graph.
$$\begin{align}
\pdv{l}{\mu_{\B}} &amp;= \sum_i \pdv{l}{x_i} \pdv{x_i}{\mu_{\B}} \\
&amp;= \sum_i \pdv{l}{x_i} \cdot \frac{-1}{\sqrt{\sigma^2_{\B} + \epsilon}} \\
\pdv{l}{\sigma^2_{\B}} &amp;= \sum_i \pdv{l}{x_i} \pdv{x_i}{\sigma^2_{\B}} \\
&amp;= \sum_i \pdv{l}{x_i} \cdot \frac{-1}{2} \cdot \frac{(x_i - \mu_{\B})}{\left(\sigma^2_{\B} + \epsilon\right)^{3/2}} \\
\pdv{l}{x_i} &amp;= \pdv{l}{\hat{x}_i}\pdv{\hat{x}_i}{x_i} + \pdv{l}{\sigma^2_{\B}}\pdv{\sigma^2_{\B}}{x_i} + \pdv{l}{\mu_{\B}}\pdv{\mu_{\B}}{x_i} \\
&amp;= \pdv{l}{\hat{x}_i}\frac{1}{\sqrt{\sigma_{\B}^2 + \epsilon}} + \pdv{l}{\sigma^2_{\B}}\frac{2(x_i-\mu_{\B})}{m} + \pdv{l}{\mu_{\B}}\frac{1}{m}
\end{align}$$</p>
<p>This is what the paper mentions as well. With a few modifications, the derivatives
here can be vectorized: <strike>$\pdv{l}{\alpha}$ would become $\grad_\alpha l$, and
the other derivatives would become jacobians. The division would become pointwise,
and so would the multiplication at some places.</strike> EDIT: I realized that
it’s easier to do this in a component-wise fashion: $x_i$ would become $x_i^{(j)}$,
the $j$th component of the $i$th example in the batch. This is also how
<a href="https://en.wikipedia.org/wiki/Batch_normalization">Wikipedia</a> does it.</p>
<h3 id="deeper-intuition">Deeper Intuition</h3>
<p>This very nice paper by <a href="https://arxiv.org/pdf/1805.10694.pdf">Kohler et al</a> shows
that Batch Normalization (and friends) may be thought of as a <em>reparameterization
of the weight space</em>. In matrix notation, The normalization operation amounts
to computing the following:</p>
<p>$$\text{BN}(x^TW) = \gamma \frac{x^TW - \text{E}(x^TW)}{\text{Var}(x^TW)^{1/2}} + \beta$$</p>
<p>Assuming $x$ is zero-mean, we can rewrite the variance as</p>
<p>$$\begin{align}
\text{Var}(x^TW) &amp;= \text{E}\left((x^TW)^2\right) \\
&amp;= \text{E}(W^Txx^TW) \\
&amp;= W^T \text{E}(xx^T) W \\
&amp;= W^T S W
\end{align}$$</p>
<p>where S is the covariance matrix of $x$. If we rewrite the weights as $\hat{W} = \gamma \frac{W}{(W^T S W)^{1/2}}$,
then the output of the layer after batch normalization is simply $x^T \hat{W}$.
Thus, we’ve reparameterized the weight matrix, making it account only for the
direction in the covariance space $S$ and letting $\gamma$ determine the
magnitude. <a href="https://arxiv.org/pdf/1602.07868.pdf">Weight Normalization</a> does the
same thing, but $S$ is replaced by $I$.</p>
<h3 id="does-it-work">Does it work?</h3>
<p>According to newer work, yes but no. <a href="https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf">Santurkar et al</a>
show that Internal Covariate Shift is not as detrimental to learning as thought
of, by synthetically injecting noise <em>after</em> BatchNorm layers. The result was
that the network doesn’t do as poorly as expected, but it still outperforms
the standard network.</p>
<p>
<img alt="batch norm graphs" src="/articles/2023/res/batch_norm_noisy_graphs.png"/>
</p>
<p>The explanation that they gave was that Batch norm assists the optimizer by
making the loss landscape smoother. Formally, the magnitude of the gradient of
the loss $||\grad_{y_j}\mathcal{L}||$ captures the smoothness of the loss.
Batch norm reduces the bound on the gradient relative to an un-batch normed
network significantly.</p>
<p>
<img alt="batch norm inequality" src="/articles/2023/res/batch_norm_ineq.png"/>
</p>
<p>Batch norm also imposes some second-order constraints on the hessian, which can
intuitively be summarized as saying that the step we take in the direction
of the gradient is more likely to lead us to a minima for a batch-normalized
network compared to an unnormalized network.</p>
<hr/>
<p>I’m surprised that people actually read these ^_^ I haven’t posted much due to
minors and assignments, but I’ll see if I can post more ML stuff on here if
time permits.</p>
</section>
<footer class="mt-12 flex flex-wrap">
<a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href="https://aniruddhadeb.com/tags/mathematics">Mathematics</a>
<a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href="https://aniruddhadeb.com/tags/machine-learning">Machine Learning</a>
<a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href="https://aniruddhadeb.com/tags/deep-learning">Deep Learning</a>
</footer>
<nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
<a class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href="https://aniruddhadeb.com/articles/2023/mac-setup-2/"><span class="mr-1.5">←</span><span>Mac Setup v2</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href="https://aniruddhadeb.com/articles/2023/l2-regularization/"><span>L2 regularization intuition</span><span class="ml-1.5">→</span></a>
</nav>
<div class="mt-24" id="disqus_thread"></div>
<script>
    const disqusShortname = 'aniruddha-deb';
    const script = document.createElement('script');
    script.src = 'https://' + disqusShortname + '.disqus.com/embed.js';
    script.setAttribute('data-timestamp', +new Date());
    document.head.appendChild(script);
  </script>
</article>
</main>
<footer class="opaco mx-auto flex h-[4.5rem] max-w-4xl items-center px-8 text-[0.9em] opacity-60">
<div class="mr-auto">
    © 2024
    <a class="link" href="https://aniruddhadeb.com">Aniruddha Deb</a>
</div>
<a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank">Built with Hugo️️</a>️
  <a class="link" href="https://github.com/Aniruddha-Deb/hugo-paper" rel="noopener" target="_blank">✎ Paper</a>
</footer>
</body>
</html>
