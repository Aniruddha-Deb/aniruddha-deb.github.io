
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://aniruddhadeb.com/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://aniruddhadeb.com/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css">


    <link href="https://aniruddhadeb.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Aniruddha Deb Atom">

    <link href="https://aniruddhadeb.com/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Aniruddha Deb RSS">


<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-79245932-2', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js' type='text/javascript'></script>  

<meta name="author" content="Aniruddha Deb" />
<meta name="description" content="$$ \require{physics} \newcommand{B}{\mathcal{B}}$$ Batch Normalization was proposed by Ioffe and Szegedy in 2015, and it spawned several normalization techniques that are used in SOTA models today (layer norm, weight norm, etc). Batch normalization normalizes the output of each layer based on the mean and variance of the …" />
<meta name="keywords" content="Mathematics, Machine Learning, Deep Learning">

<meta property="og:site_name" content="Aniruddha Deb"/>
<meta property="og:title" content="Batch Normalization"/>
<meta property="og:description" content="$$ \require{physics} \newcommand{B}{\mathcal{B}}$$ Batch Normalization was proposed by Ioffe and Szegedy in 2015, and it spawned several normalization techniques that are used in SOTA models today (layer norm, weight norm, etc). Batch normalization normalizes the output of each layer based on the mean and variance of the …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://aniruddhadeb.com/articles/2023/batch-normalization.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2023-02-16 16:00:00+05:30"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://aniruddhadeb.com/author/aniruddha-deb.html">
<meta property="article:section" content="Mathematics"/>
<meta property="article:tag" content="Mathematics"/>
<meta property="article:tag" content="Machine Learning"/>
<meta property="article:tag" content="Deep Learning"/>
<meta property="og:image" content="/extras/sitelogo.png">

  <title>Aniruddha Deb &ndash; Batch Normalization</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://aniruddhadeb.com">
        <img src="/extras/sitelogo.png" alt="" title="">
      </a>
      <h1><a href="https://aniruddhadeb.com"></a></h1>


      <nav>
        <ul class="list">
          <li><a href="https://aniruddhadeb.com/pages/about.html#about">About</a></li>
          <li><a href="https://aniruddhadeb.com/pages/feeds.html#feeds">Feeds</a></li>

          <li><a href="/archives.html">archives</a></li>
          <li><a href="/categories">categories</a></li>
          <li><a href="/tags">tags</a></li>
          <li><a href="https://www.cse.iitd.ac.in/~cs1200869">website</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="https://www.github.com/Aniruddha-Deb" target="_blank"><i class="fab fa-github"></i></a></li>
        <li><a class="sc-stack-exchange" href="https://stackexchange.com/users/12827944/aniruddha-deb" target="_blank"><i class="fab fa-stack-exchange"></i></a></li>
        <li><a class="sc-twitter" href="https://www.twitter.com/hairband_dude" target="_blank"><i class="fab fa-twitter"></i></a></li>
        <li><a class="sc-envelope-o" href="mailto:aniruddha.deb.2002@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="batch-normalization">Batch Normalization</h1>
    <p>
          Posted on Thu 16 February 2023 in <a href="https://aniruddhadeb.com/category/mathematics.html">Mathematics</a>


    </p>
  </header>


  <div>
    <p>$$
\require{physics}
\newcommand{B}{\mathcal{B}}$$</p>
<p>Batch Normalization was proposed by <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf">Ioffe and Szegedy</a>
in 2015, and it spawned several normalization techniques that are used in SOTA
models today (layer norm, weight norm, etc). Batch normalization normalizes
the output of each layer based on the mean and variance of the examples in the
current batch. Formally, if $\B = {x_1, \ldots, x_m}$ is our batch, then 
batch norm does the following transformation:</p>
<p>$$\begin{align}
\mu_{\B} &amp;= \frac{1}{m} \sum_i x_i \\
\sigma_{\B}^2 &amp;= \left( \frac{1}{m} \sum_i x_i^2 \right) - \mu_{\B}^2 \\
\hat{x}_i &amp;= \frac{x_i - \mu_{\B}}{\sqrt{\sigma^2_{\B} + \epsilon}} \\
y_i &amp;= \gamma \hat{x}_i + \beta 
\end{align}$$</p>
<h3>Why?</h3>
<p>When networks are trained, the partial derivative of a parameter obtained 
during backprop gives us the gradient <em>assuming all other parameters remain
constant</em>. This is not true, as the parameters simultaneously change. This 
causes the input distribution to a layer in a deep neural network to change
significantly from update to update, and this is called <em>Internal Covariate 
Shift</em>.</p>
<p>Mathematically, if we have a simple linear model $y = w_l w_{l-1} \ldots w_1 x$, and
we update each parameter via the partial derivative of that parameter, then we
get $\hat{y} = (w_l - \epsilon g_l) \ldots (w_1 - \epsilon g_1) x$. Note how even the
second order terms ($\epsilon^2 (\prod_{i=l}^3 w_i)g_2 g_1$) may become very large,
making learning harder.</p>
<p>Batch norm aims to reduce internal covariate shift by normalizing the layers.
This centers them and reduces variance due to noisy parameter updates, while not
taking any information away (the model is still free to learn whatever mean
and variance it chooses to learn by updating $\gamma$ and $\beta$.)</p>
<h3>Backprop</h3>
<p>Before manually doing the backprop, a computation graph really helps here.</p>
<p><img alt="comp_graph" src="res/batch_norm_graph.jpg"></p>
<p>This is a small computation graph that uses just three examples, but it's
sufficient to show which variables are dependent on each other, and how. This
allows us to go ahead and obtain all the partial derivatives via chain rule:</p>
<p>$$\begin{align}
\pdv{l}{\gamma} &amp;= \sum_i \pdv{l}{y_i} \hat{x}_i \\
\pdv{l}{\beta} &amp;= \sum_i \pdv{l}{y_i} \\
\pdv{l}{\hat{x}_i} &amp;= \pdv{l}{y_i} \gamma \\
\end{align}$$</p>
<p>The next three are a bit tricky. To check the derivation, just follow the edges
of the computation graph.
$$\begin{align}
\pdv{l}{\mu_{\B}} &amp;= \sum_i \pdv{l}{x_i} \pdv{x_i}{\mu_{\B}} \\
                  &amp;= \sum_i \pdv{l}{x_i} \cdot \frac{-1}{\sqrt{\sigma^2_{\B} + \epsilon}} \\
\pdv{l}{\sigma^2_{\B}} &amp;= \sum_i \pdv{l}{x_i} \pdv{x_i}{\sigma^2_{\B}} \\
                       &amp;= \sum_i \pdv{l}{x_i} \cdot \frac{-1}{2} \cdot \frac{(x_i - \mu_{\B})}{\left(\sigma^2_{\B} + \epsilon\right)^{3/2}} \\
\pdv{l}{x_i} &amp;= \pdv{l}{\hat{x}_i}\pdv{\hat{x}_i}{x_i} + \pdv{l}{\sigma^2_{\B}}\pdv{\sigma^2_{\B}}{x_i} + \pdv{l}{\mu_{\B}}\pdv{\mu_{\B}}{x_i} \\
             &amp;= \pdv{l}{\hat{x}_i}\frac{1}{\sqrt{\sigma_{\B}^2 + \epsilon}} + \pdv{l}{\sigma^2_{\B}}\frac{2(x_i-\mu_{\B})}{m} + \pdv{l}{\mu_{\B}}\frac{1}{m} 
\end{align}$$</p>
<p>This is what the paper mentions as well. With a few modifications, the derivatives
here can be vectorized: <strike>$\pdv{l}{\alpha}$ would become $\grad_\alpha l$, and
the other derivatives would become jacobians. The division would become pointwise,
and so would the multiplication at some places.</strike> EDIT: I realized that
it's easier to do this in a component-wise fashion: $x_i$ would become $x_i^{(j)}$,
the $j$th component of the $i$th example in the batch. This is also how
<a href="https://en.wikipedia.org/wiki/Batch_normalization">Wikipedia</a> does it.</p>
<h3>Deeper Intuition</h3>
<p>This very nice paper by <a href="https://arxiv.org/pdf/1805.10694.pdf">Kohler et al</a> shows
that Batch Normalization (and friends) may be thought of as a <em>reparameterization
of the weight space</em>. In matrix notation, The normalization operation amounts
to computing the following:</p>
<p>$$\text{BN}(x^TW) = \gamma \frac{x^TW - \text{E}(x^TW)}{\text{Var}(x^TW)^{1/2}} + \beta$$</p>
<p>Assuming $x$ is zero-mean, we can rewrite the variance as</p>
<p>$$\begin{align}
\text{Var}(x^TW) &amp;= \text{E}\left((x^TW)^2\right) \\
                 &amp;= \text{E}(W^Txx^TW) \\
                 &amp;= W^T \text{E}(xx^T) W \\
                 &amp;= W^T S W
\end{align}$$</p>
<p>where S is the covariance matrix of $x$. If we rewrite the weights as $\hat{W} = \gamma \frac{W}{(W^T S W)^{1/2}}$,
then the output of the layer after batch normalization is simply $x^T \hat{W}$.
Thus, we've reparameterized the weight matrix, making it account only for the
direction in the covariance space $S$ and letting $\gamma$ determine the 
magnitude. <a href="https://arxiv.org/pdf/1602.07868.pdf">Weight Normalization</a> does the
same thing, but $S$ is replaced by $I$.</p>
<h3>Does it work?</h3>
<p>According to newer work, yes but no. <a href="https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf">Santurkar et al</a>
show that Internal Covariate Shift is not as detrimental to learning as thought
of, by synthetically injecting noise <em>after</em> BatchNorm layers. The result was
that the network doesn't do as poorly as expected, but it still outperforms 
the standard network.</p>
<p><img alt="batch norm graphs" src="res/batch_norm_noisy_graphs.png"></p>
<p>The explanation that they gave was that Batch norm assists the optimizer by
making the loss landscape smoother. Formally, the magnitude of the gradient of
the loss $||\grad_{y_j}\mathcal{L}||$ captures the smoothness of the loss.
Batch norm reduces the bound on the gradient relative to an un-batch normed 
network significantly.</p>
<p><img alt="batch norm inequality" src="res/batch_norm_ineq.png"></p>
<p>Batch norm also imposes some second-order constraints on the hessian, which can
intuitively be summarized as saying that the step we take in the direction
of the gradient is more likely to lead us to a minima for a batch-normalized
network compared to an unnormalized network.</p>
<hr>
<p>I'm surprised that people actually read these ^_^ I haven't posted much due to
minors and assignments, but I'll see if I can post more ML stuff on here if
time permits.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://aniruddhadeb.com/tag/mathematics.html">Mathematics</a>
      <a href="https://aniruddhadeb.com/tag/machine-learning.html">Machine Learning</a>
      <a href="https://aniruddhadeb.com/tag/deep-learning.html">Deep Learning</a>
    </p>
  </div>





<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'aniruddha-deb';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
        Please enable JavaScript to view comments.

</noscript>
<!-- End Disqus -->
</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Aniruddha Deb ",
  "url" : "https://aniruddhadeb.com",
  "image": "/extras/sitelogo.png",
  "description": ""
}
</script>

</body>
</html>